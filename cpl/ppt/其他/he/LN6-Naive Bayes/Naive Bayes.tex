%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[8pt]{beamer}
\usepackage{CJK}
\usepackage{ctex}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{longtable}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathtools}
\usepackage{animate}
\usepackage{amsthm, amsmath}
 
%\usepackage{media9}
%% A LATEX package for embedding interactive Adobe Flash (SWF) and 3D files (Adobe U3D & PRC) as well as video and sound files or streams (FLV, MP4/H.246, MP3) into PDF documents with Adobe Reader-9/X
%compatibility.
\renewcommand{\algorithmicrequire}{\textbf{Input:}}   %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{\textbf{Output:}}  %UseOutput in the format of Algorithm
\newcommand{\e}[1]{\ensuremath{\times 10^{#1}}}
%\mode<presentation>{\usetheme{Madrid}}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\begin{document}
\begin{CJK*}{GBK}{kai}
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Machine Learning]{Bayes Classifier and Naive Bayes} % The short title appears at the bottom of every slide, the full title is only on the title page


\author{Kun He (ºÎçû)} % Your name
%\logo{%
%   \includegraphics[scale=.2]{logo.pdf}\hspace*{4.75cm}~%
%   \includegraphics[scale=.2]{logo.jpg}\hspace*{0.75cm}%
%   }
%\pgfdeclareimage[width=1cm]{hust}{logo.pdf}
%\logo{\pgfuseimage{hust}{\vspace{-10pt}}}
\titlegraphic{\includegraphics[width=1.3cm]{logo.pdf}}
\institute[JHL, HUST] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
	Data Mining and Machine Learning Lab\\
	(John Hopcroft Lab)\\
	Huazhong University of Science \& Technology \\ % Your institution for the title page
	\medskip
	\textit{brooklet60@hust.edu.cn} % Your email address
}

\date{2022Äê5ÔÂ} % Date, can be changed to a custom date

%====================================================
\frame{\titlepage}

\frame{\frametitle{Table of Contents}\tableofcontents}

\AtBeginSection[]
{
\begin{frame}{Table of Contents}
\tableofcontents[currentsection]
\end{frame}
}

%------------------------------------------------
%------------------------------------------------
\section{Basic Idea}
%------------------------------------------------
\subsection{Introduction}
\begin{frame}
\frametitle{Introduction}
\begin{block}{Basic idea}
	In machine learning, the Naive Bayes classifier is a series of simple probability classifiers based on the Bayesian theorem under strong independent assumptions.
\end{block}

\begin{itemize}
	\item Training Data:$D$ =  \{($\mathbf{x}_1$,$y_1$),...,($\mathbf{x}_n$,$y_n$)\},($\mathbf{x}_i$,$y_i$) is sampled i.i.d from unknown distribution $P(X,Y)$.So we obtain:
	\[ P(D)=P ((\mathbf{x}_1,y_1),...,(\mathbf{x}_n,y_n)) = \prod_{\alpha=1}^{n} P(\mathbf{x}_\alpha,y_\alpha). \]
	\item If we do have enough data, we could estimate $P(X,Y)$ similar to the coin example in the previous lecture, where we imagine a gigantic die that has one side for each possible value of $(x,y)$. We can estimate the probability that one specific side comes up through counting.
	\item Estimate $P(X,Y)$:
	\[\hat P(\mathbf{x},y)=\frac{\sum_{i=1}^{n} I(\mathbf{x}_i = x \wedge y_i = y)}{n}.\]
	\[I(\mathbf{x}_i=x \wedge y_i=y)=1\quad  if\quad  \mathbf{x}_i=x \quad and\quad y_i=y, \] and 0 otherwise.	
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.15]{Die.png}
\end{figure}
\end{frame}
%------------------------------------------------
\begin{frame}
	\frametitle{Basic idea}
\begin{itemize}	
    \item Of course, if we are primarily interested in predicting the label $y$ from the features $x$, we may estimate $P(Y|X)$ directly instead of $P(X,Y)$. We can then use the Bayes Optimal Classifier for a specific $\hat{P}(y|x)$ to make predictions.
	
	\item %Estimate $P(y|\mathbf{x})$:predict the label $y$ from the features $\mathbf{x}$
	We can then use the Bayes Optimal Classifier for a specific $\hat{P}(y|x)$ to make predictions.
\[\hat{P}(y|\mathbf{x}) = \frac{\hat{P}(y,\mathbf{x})}{P(\mathbf{x})} = \frac{[\sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x} \wedge {y}_i = y)]/n}{ [\sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x})]/n}. \]
\[ ~~~~~~~~~~~~~~  = \frac{\sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x} \wedge {y}_i = y)}{ \sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x})}     \]
%\item Regression rule: The output is the property value for the object. This value is the average of the values of \textbf{k} nearest neighbors.
\end{itemize}
\end{frame}
%------------------------------------------------


\begin{frame}
\frametitle{Visualization}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{Venn_diagram}
	%\caption{Venn diagram}
\end{figure}
\begin{block}{Venn diagram}
\begin{itemize}
\item The Venn diagram illustrates that the MLE method estimates:
\[\hat{P}(y|\mathbf{x}) = \frac{|C|}{|B|}. \]
\end{itemize}
\end{block}
\end{frame}
%------------------------------------------------
%------------------------------------------------
\section{Naive Bayes}
%------------------------------------------------
\subsection{Bayes Rule}
\begin{frame}
\frametitle{Bayes rule}
If we can estimate $P$($y$) and $P$($\mathbf{x}$ $|$ $y$),since, by Bayes rule,
\[P(y | \mathbf{x}) = \frac{P(\mathbf{x} | y)P(y)}{P(\mathbf{x})}.\]

\begin{block}{Estimating $P(y$), $P(x|y$)}
\begin{itemize}
	\item Estimating $P$($y$) is easy: For example, if $Y$ takes on discrete binary values estimating $P$($Y$) reduces to coin tossing. We simply need to count how many times we observe each outcome (in this case each class):
	\[P(y = c)  = \frac{\sum_{i=1}^{n} I(y_i = c)}{n} = \hat\pi_c\]
	\item Estimating $P(x|y)$, however, is \textbf{not easy}! E.g. Spam checker.
	\item The additional assumption that we make is the Naive Bayes assumption.
\end{itemize}
\end{block}

\end{frame}
%------------------------------------------------
\subsection{Naive Bayes Assumption}
\begin{frame}
\begin{block}{Naive Bayes Assumption:}
\[P(\mathbf{x} | y) = \prod_{\alpha = 1}^{d} P(x_\alpha | y), \text{where } x_\alpha = [\mathbf{x}]_\alpha \text{ is the value for feature } \alpha .\]
i.e.,feature values are \textbf{independent given the label!} This is a very \textbf{bold} assumption. 
\end{block}
For example, a setting where the Naive Bayes classifier is often used is spam filtering. Here, the data is emails and the label is spam or not-spam. The Naive Bayes assumption implies that the words in an email are conditionally independent, given that you know that an email is spam or not. Clearly this is not true. Neither the words of spam or not-spam emails are drawn independently at random. However, the resulting classifiers can work well in practice even if this assumption is violated.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{NaiveBayes.png}
\end{figure}

\end{frame}

%------------------------------------------------
 
\begin{frame}
	So, for now, let's pretend the Naive Bayes assumption holds. Then the Bayes Classifier can be defined as follows. 
	\frametitle{Estimating $P$($\mathbf{x}$ $|$ $y$)}
	\begin{block}{Bayes Classifier}
		Because of the Naive Bayes assumption
		\begin{align}
		h(\mathbf{x}) &= \operatorname*{argmax}_y P(y | \mathbf{x}) \\
		&= \operatorname*{argmax}_y \; \frac{P(\mathbf{x} | y)P(y)}{P(\mathbf{x})} \\
		&= \operatorname*{argmax}_y \; P(\mathbf{x} | y) P(y) && \text{($P(\mathbf{x})$ does not depend on $y$)} \\
		&= \operatorname*{argmax}_y \; \prod_{\alpha=1}^{d} P(x_\alpha | y) P(y) && \text{(by the naive Bayes assumption)}\\
		&= \operatorname*{argmax}_y \; \sum_{\alpha = 1}^{d} \log(P(x_\alpha | y)) + \log(P(y)) && \text{(as log is a monotonic function)}
		\end{align}
	\end{block}
Estimating $log(P(x_{\alpha}|y))$ is easy as we only need to consider one dimension. And estimating $P(y)$ is not affected by the assumption.
\end{frame}
%%------------------------------------------------


%%------------------------------------------------
\section{Estimating $P$([$\mathbf{x}]_\alpha$ $|$ $y$)}
%------------------------------------------------
\subsection{Case \# 1: Categorical Features}
\begin{frame}
\frametitle{Case \# 1: Categorical features}
Features:
\[ [\mathbf{x}]_\alpha \in \{f_1, f_2, \cdots, f_{K_\alpha}\}.\]
\\
²»Í¬µÄ$[\mathbf{x}]_\alpha$, ÈçÐÔ±ð£¬ÄêÁä£¬Ê¡·Ý; 
\\
Each feature $\alpha$ falls into one of $K_\alpha$ categories. (Note that the case with binary features is just a specific case of this, where $K_\alpha=2$.) An example of such a setting may be medical data where one feature could be gender (male / female) or marital status (single / married ). \\

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.35]{CategoricalNB.png}
\end{figure}
\end{frame}
%------------------------------------------------
\begin{frame}
	\frametitle{Categorical features}
²»Í¬µÄ$[\mathbf{x}]_\alpha$, ÈçÐÔ±ð£¬ÄêÁä£¬Ê¡·Ý;  $y =c $, Èç½¡¿µ×´Ì¬
\\
~~~
\\
	\textbf{Model $P$($x_\alpha$ $|$ $y$)}:
	\[P(x_{\alpha} = j | y=c) = [\theta_{jc}]_{\alpha} \text{ and } \sum_{j=1}^{K_\alpha} [\theta_{jc}]_{\alpha} = 1.\]
	$[\theta_{jc}]_{\alpha}$ is the probability of feature $\alpha$ having the value j, given that the label is $c$. And the constraint indicates that $x_{\alpha}$ must have one of the categories $\{1, \dots, K_\alpha\}$.
	
	
	\textbf{Parameter Estimation}:
	\begin{align}
	[\hat\theta_{jc}]_{\alpha} &= \frac{\sum_{i=1}^{n} I(y_i = c) I(x_{i\alpha} = j) + \ell}{\sum_{i=1}^{n} I(y_i = c) + \ell K_\alpha},
	\end{align}
	\[ x_{i\alpha} = [\mathbf{x}_i]_\alpha,\]
	$l$ is a smoothing parameter. \\
	By setting $\ell$=0 we get an MLE estimator, $\ell$ $>$ 0 leads to MAP. \\
	If we set $\ell$=+1 we get Laplace smoothing.\\
	In words,this means:
	\[\frac{\text{ number of samples with label c that have feature } \alpha \text{ with value $j$ }}{\text{ number of samples with label $c$}}.\]
	Àý£º cÎª»¼¹ÚÐÄ²¡£¬$\alpha$ÎªÄêÁä¶Î£¬ $j$Îª60ÖÁ69
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Prediction}
Essentially the categorical feature model associate a special coin with each feature and label. The generative model that we are assuming is that the data was generated by first choosing the label (e.g. "healthy person"). That label comes with a set of d "dice", for each dimension one. The generator picks each die, tosses it and fills in the feature value with the outcome of the coin toss. So if there are C possible labels and d dimensions we are estimating $d \times C$ "dice" from the data. However, per data point only $d$ dice are tossed (one for each dimension). Die $\alpha$ (for any label) has $K_\alpha$ possible "sides". Of course this is not how the data is generated in reality - but it is a modeling assumption that we make. We then learn these models from the data and during test time see which model is more likely given the sample.\\

\textbf{Prediction}
\[h(\mathbf{x}) = \operatorname*{argmax}_y \; \prod_{\alpha=1}^{d} P(x_\alpha | y) P(y) \]
\[\operatorname*{argmax}_y \; P(y=c \mid \mathbf{x}) \propto \operatorname*{argmax}_y \; \hat\pi_c \prod_{\alpha = 1}^{d} [\hat\theta_{jc}]_\alpha \]
\[\hat\pi_c = P(y = c)  = \frac{\sum_{i=1}^{n} I(y_i = c)}{n} \]
\end{frame}
%------------------------------------------------

%------------------------------------------------

%------------------------------------------------
\subsection{Case \# 2: Multinomial Features}
\begin{frame}
\frametitle{Case \# 2: Multinomial features}
\begin{block}{Multinomial features}
If feature values don't represent categories (e.g. male/female) but counts, we need to use a different model. E.g. in the text document categorization, feature value $x_\alpha$=$j$ means that in this particular document $\mathbf{x}$ the $\alpha^{th}$ word in my dictionary appears $j$ times.\\
Let us consider the example of spam filtering. Imagine the $\alpha^{th}$ word is indicative towards $spam$.Then if $x_{\alpha}$=10 means that this email is likely spam(as word $\alpha$ appears 10 times in it). And another email with $x'_{\alpha}$=20 should be even more likely to be spam (as the spammy word appears twice as often). With categorical features this is not guaranteed.
\end{block}
\underline{Features:}
µÚ$\alpha$ÖÖµ¥´Ê³öÏÖµÄÊýÁ¿£º
\begin{align}
	x_\alpha \in \{0, 1, 2, \dots, m\} \text{ and } m = \sum_{\alpha = 1}^d x_\alpha
\end{align}
Each feature $\alpha$ represents a count and $m$ is the length of the sequence. An example of this could be the count of a specific word $\alpha$ in a document of length $m$ and $d$ is the size of the vocabulary.
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Model P($\mathbf{x}$$|$$y$)}
Use the multinomial distribution:
\[P(\mathbf{x} \mid m, y=c) = \frac{m!}{x_1! \cdot x_2! \cdot \dots \cdot x_d!} \prod_{\alpha = 1}^d\left(\theta_{\alpha c}\right)^{x_\alpha}\]
where $\theta_{\alpha c}$ is the probability of selecting $x_\alpha$ and $\sum_{\alpha = 1}^d \theta_{\alpha c} =1$.\\
 So, we can use this to generate a spam email, i.e., a document $\mathbf{x}$ of class $y = \text{spam}$ by picking $m$ words independently at random from the vocabulary of $d$ words using $P(\mathbf{x} \mid y = \text{spam})$.\\
\underline{Parameter estimation:}
\begin{align}
	\hat\theta_{\alpha c} = \frac{\sum_{i = 1}^{n} I(y_i = c) x_{i\alpha} + \ell}{\sum_{i=1}^{n} I(y_i = c) m_i + \ell \cdot d }
\end{align}
where $m_i=\sum_{\beta = 1}^{d} x_{i\beta}$ denotes the number of words in document $i$.The numerator sums up all counts for feature $x_\alpha$ and the denominator sums up all counts of all features across all data points. \\
In words:
\[\frac{\text{number of times word } \alpha \text{ appears in all spam emails}}{\text{ number of words in all spam emails combined}}.\]
\underline{Prediction:}
\[\operatorname*{argmax}_c \; P(y = c \mid \mathbf{x}) \propto \operatorname*{argmax}_c \; \hat\pi_c \prod_{\alpha = 1}^d \hat\theta_{\alpha c}^{x_\alpha}\]
\end{frame}
%------------------------------------------------
\subsection{Case \#3: Continuous Features (Gaussian Naive Bayes)}
\begin{frame}
\frametitle{Case \#3: Continuous features (Gaussian Naive Bayes)}
\underline{Features:} ÈçÉí¸ß£¬ÑªÑ¹£¬
\begin{align}
	x_\alpha \in \mathbb{R} && \text{(each feature takes on a real value)}
\end{align}
\underline{Model} $P(x_\alpha \mid y)$ Use Gaussian distribution:
\begin{align}
	P(x_\alpha \mid y=c) = \mathcal{N}\left(\mu_{\alpha c}, \sigma^{2}_{\alpha c}\right) = \frac{1}{\sqrt{2 \pi} \sigma_{\alpha c}} e^{-\frac{1}{2} \left(\frac{x_\alpha - \mu_{\alpha c}}{\sigma_{\alpha c}}\right)^2}
\end{align}
Note that the model specified above is based on our assumption about the data - that each feature $\alpha$ comes from a class-conditional Gaussian distribution.
The full distribution:
\[P(\mathbf{x}|y)\sim \mathcal{N}(\mathbf{\mu}_y,\Sigma_y) \]
where $\Sigma_y$  is a diagonal covariance matrix with
\[ [\Sigma_y]_{\alpha,\alpha}=\sigma^2_{\alpha,y} \]

\end{frame}
%------------------------------------------------
%------------------------------------------------
\begin{frame}
\frametitle{Parameter estimation:}
\begin{block}{Parameter estimation:}
As always, we estimate the parameters of the distributions for each dimension and class independently. Gaussian distributions only have two parameters, the mean and variance. \\
The mean $\mu_{\alpha,y}$ is estimated by the average feature value of dimension $\alpha$ from all samples with label $y$. \\
The (squared) standard deviation is simply the variance of this estimate.
\end{block}
\begin{align}
	\mu_{\alpha c} &\leftarrow \frac{1}{n_c} \sum_{i = 1}^{n} I(y_i = c) x_{i\alpha} && \text{where $n_c = \sum_{i=1}^{n} I(y_i = c)$} \\
	\sigma_{\alpha c}^2 &\leftarrow \frac{1}{n_c} \sum_{i=1}^{n} I(y_i = c)(x_{i\alpha} - \mu_{\alpha c})^2
\end{align}
\end{frame}
%------------------------------------------------
%%------------------------------------------------
\section{Naive Bayes Classifier}
%------------------------------------------------
\subsection{Naive Bayes is a Linear Classifier}
\begin{frame}
	\frametitle{Naive Bayes is a Linear Classifier}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.35]{GaussianNB.png}			
	\end{figure}
	%Do
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{1. Multinomial Features}
Suppose that $y_i \in \{-1, +1\}$ and features are multinomial. So:
\[h(\mathbf{x}) = \operatorname*{argmax}_y \; P(y) \prod_{\alpha - 1}^d P(x_\alpha \mid y) = \textrm{sign}(\mathbf{w}^\top \mathbf{x} + b) \]
\[\mathbf{w}^\top \mathbf{x} + b > 0 \Longleftrightarrow h(\mathbf{x}) = +1.\]
As before, we define:
\[P(x_\alpha|y=+1)\propto\theta_{\alpha+}^{x_\alpha};P(Y=+1)=\pi_+.\]
\begin{align}
	[\mathbf{w}]_\alpha &= \log(\theta_{\alpha +}) - \log(\theta_{\alpha -}) \\
	b &= \log(\pi_+) - \log(\pi_-)
\end{align}
If we use the above to do classification, we can compute for $\mathbf{w}^\top \mathbf{x} + b$
\end{frame}
%------------------------------------------------
\begin{frame}
\begin{align}
		\mathbf{w}^\top \mathbf{x} + b > 0 &\Longleftrightarrow \sum_{\alpha = 1}^{d} [\mathbf{x}]_\alpha
		\overbrace{(\log(\theta_{\alpha +}) - \log(\theta_{\alpha -}))}^{[\mathbf{w}]_\alpha} + \overbrace{\log(\pi_+) - \log(\pi_-)}^b > 0  \\
		&\Longleftrightarrow \exp\left(\sum_{\alpha = 1}^{d} [\mathbf{x}]_\alpha
		{(\log(\theta_{\alpha +}) - \log(\theta_{\alpha -}))} + {\log(\pi_+) - \log(\pi_-)} \right)> 1  \\
		&\Longleftrightarrow \prod_{\alpha = 1}^{d}
		\frac{\exp\left( \log\theta_{\alpha +}^{[\mathbf{x}]_\alpha} + \log(\pi_+)\right)}
		{\exp\left(\log\theta_{\alpha -}^{[\mathbf{x}]_\alpha} + \log(\pi_-)\right)}
		> 1   \\
		&\Longleftrightarrow \prod_{\alpha = 1}^{d}
		\frac{\theta_{\alpha +}^{[\mathbf{x}]_\alpha} \pi_+}
		{\theta_{\alpha -}^{[\mathbf{x}]_\alpha} \pi_-} > 1   \\
		&\Longleftrightarrow \frac{\prod_{\alpha = 1}^{d} P([\mathbf{x}]_\alpha | Y = +1)\pi_+}{\prod_{\alpha =1}^{d}P([\mathbf{x}]_\alpha | Y = -1)\pi_-} > 1 \\
		&\Longleftrightarrow \frac{P(\mathbf{x} | Y = +1)\pi_+}{P(\mathbf{x} | Y = -1)\pi_-} > 1 \\
		&\Longleftrightarrow \frac{P(Y = +1 |\mathbf{x})}{P( Y = -1|\mathbf{x})}>1  \\
		&\Longleftrightarrow P(Y = +1 | \mathbf{x}) > P(Y = -1 | \mathbf{x})  \\
		&\Longleftrightarrow \operatorname*{argmax}_y  P(Y=y|\mathbf{x})=+1
\end{align}
\end{frame}
%------------------------------------------------

%------------------------------------------------
\subsection{Gaussian Naive Bayes}
\begin{frame}
\frametitle{Gaussian Naive Bayes}
\begin{block}{Gaussian Naive Bayes}
In the case of continuous features (Gaussian Naive Bayes), we can show that:
\[P(y \mid \mathbf{x}) = \frac{1}{1 + e^{-y (\mathbf{w}^\top \mathbf{x} +b) }} \]
This model is also known as \textbf{logistic regression}.
\end{block}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{sigmoid.png}
	\caption{If ¡®t¡¯ goes to infinity, y(predicted) will become 1 and if ¡®t¡¯ goes to negative infinity, y(predicted) will become 0.	
	}
\end{figure}
\end{frame}

%------------------------------------------------
\subsection{Gaussian Naive Bayes}
\begin{frame}
	\frametitle{Gaussian Naive Bayes}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.4]{GNBisLR.png}
	\end{figure}
\end{frame}

%%------------------------------------------------
\section{Examples and Application}
%------------------------------------------------
\subsection{Filter Spam with Naive Bayes}
\begin{frame}
\frametitle{Filter spam with naive bayes}
\begin{block}{Core algorithm:Naive Bayesian classifier training function}
	def trainNB0(trainMatrix, trainCategory):¼ÆËãÑµÁ·µÄÎÄµµÊýÄ¿\\
	\quad numTrainDocs = len(trainMatrix)¼ÆËãÎÄµµµÄ´ÊÌõÊý\\
	\quad numWords = len(trainMatrix[0])ÎÄµµÊôÓÚÀ¬»øÓÊ¼þÀàµÄ¸ÅÂÊ\\
	\quad pAbusive = sum(trainCategory)/float(numTrainDocs)³õÊ¼»¯\\
    \quad p0Num = ones(numWords); p1Num = ones(numWords) \\
    \quad p0Denom = 2.0; p1Denom = 2.0\\
    \quad for i in range(numTrainDocs):\\
	\qquad if trainCategory[i] == 1:Í³¼Æ¼ÆËã´ÊÓïÊôÓÚÀ¬»øÓÊ¼þÀàµÄÌõ¼þ¸ÅÂÊËùÐèµÄÊý¾Ý\\
	\qquad \quad p1Num += trainMatrix[i]\\
	\qquad \quad p1Denom += sum(trainMatrix[i])\\
	\qquad else:Í³¼Æ¼ÆËãÊôÓÚ·ÇÀ¬»øÓÊ¼þÀàµÄÌõ¼þ¸ÅÂÊËùÐèµÄÊý¾Ý\\
	\qquad \quad p0Num += trainMatrix[i]\\
	\qquad \quad p0Denom += sum(trainMatrix[i])\\
	\quad Ïà³ý¼ÆËã¸ÅÂÊÏòÁ¿\\
    \quad p1Vect = log(p1Num / p1Denom)\\
	\quad p0Vect = log(p0Num / p0Denom)\\
	\quad ·µ»Ø´ÊÓïÊôÓÚÀ¬»øÓÊ¼þÀàµÄÌõ¼þ¸ÅÂÊÏòÁ¿£¬´ÊÓïÊôÓÚ·ÇÀ¬»øÓÊ¼þÀàµÄÌõ¼þ¸ÅÂÊÏòÁ¿£¬ÎÄµµÊôÓÚÀ¬»øÓÊ¼þÀà¸ÅÂÊ.
	\quad return p0Vect, p1Vect, pAbusive
\end{block}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Classify}
\begin{block}{Classify}
	def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\\
	\quad ÊäÈëÎªÐèÒª·ÖÀàµÄ´ÊÏòÁ¿£¬ÒÔ¼°´ÊÓïÊôÓÚÀ¬»øÓÊ¼þÀàµÄÌõ¼þ¸ÅÂÊÏòÁ¿£¬´ÊÓïÊôÓÚ·ÇÀ¬»øÓÊ¼þÀàµÄÌõ¼þ¸ÅÂÊÏòÁ¿£¬ÎÄµµÊôÓÚÀ¬»øÓÊ¼þÀàµÄ¸ÅÂÊ\\
    \quad p1=sum(vec2Classify*p1Vec)+log(pClass1)\\
    \quad p0=sum(vec2Classify*p0Vec)+log(1.0-pClass1)\\
    \quad if p1 $>$ p0:\\
        \qquad \quad return 1\\
    \quad else:\\
		\qquad \quad return 0\\
\end{block}
Because:
\[p(c_{i} | \textbf{w}) = \frac{p(\textbf{w} | c_{i})p(c_{i})}{p(\textbf{w})},w:word \ vector;c_{i}:label\]
\[p(\textbf{w} | c_{i}) = p(w_{0},w_{1},...,w_{N} | c_{i}) = p(w_{0} | c_{i})p(w_{1} | c_{i})...p(w_{N} | c_{i})\]
\[log(p(\textbf{w} | c_{i})p(c_{i})) \]
\[= log(p(w_{0} | c_{i})p(w_{1} | c_{i})...p(w_{N} | c_{i})p(c_{i}))  \]
\[= log(p(w_{0} | c_{i})) + log(p(w_{1} | c_{i})) + ... +log(p(w_{N} | c_{i})) + log(p(c_{i})) \]
\end{frame}
%%------------------------------------------------
\section{Naive Bayes Summary}
\subsection{Summary of Naive Bayes}
\begin{frame}
\frametitle{Summary of Naive Bayes}
Bayesian formula:
\[p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)}\]
Assumption:
\[p(x_{1},x_{2},...,x_{n} | y ) = p(x_{1} | y)p(x_{2} | y)...p(x_{n} | y)\]
Likelihood function:
\[ \prod_{i=1}^{n}p(x_{i} | y,\theta) \]
Log-likelihood:
\[\sum_{i=1}^{n}log(p(x_{i} | y,\theta)) \]
Maximum likelihood estimation:
\[\operatorname*{argmax}_{\theta} \sum_{i=1}^{n}log(p(x_{i} | y,\theta)) \]
Classify:
\[ \operatorname*{argmax}_{y} p(y)\prod_{i=1}^{n}p(x_{i} | y,\theta) = \operatorname*{argmax}_{y} log(p(y)) + \sum_{i=1}^{n}log(p(x_{i} | y,\theta)) \]

\end{frame}
%------------------------------------------------

%%------------------------------------------------
%\section{Reference}
%------------------------------------------------
%\begin{frame}
%\frametitle{Reference}
%\begin{thebibliography}{4}
%\bibitem{LS-SPH} F. Zou, C. Liu, H. Ling, H. Feng, L. Yan, and D. Li, "Least square regularized spectral hashing for similarity search," Signal Processing, vol. 93, pp. 2265-2273, 2013. (SCI,EI)
%\bibitem{KMFH} F. Zou, Y. Chen, J. Song, K. Zhou, Y. Yang, and N. Sebe, "Compact image fingerprint via multiple kernel hashing," IEEE Transactions on Multimedia, vol. 17, pp. 1006-1018, 2015. (SCI,EI)
%\bibitem{KNPH} C. Liu, H. Ling, F. Zou, L. Yan, Y. Wang, H. Feng, et al., "Kernelized neighborhood preserving hashing for social-network-oriented digital fingerprints," IEEE Transactions on Information Forensics and Security, vol. 9, pp. 2232-2247, 2014. (SCI,EI)
%\bibitem{DTSH} Liu, Yu; Song, Jingkuan; Zhou, Ke; Yan, Lingyu; Liu, Li; Zou, Fuhao; Shao, Ling, "Deep Self-taught Hashing for Image Retrieval," IEEE Transactions on Cybernetics, May 3, 2018. (SCI,EI)
%\bibitem{DeepFace} Fuhao Zou, Fan Yang, Wei Chen,Kai Lia, Jingkuan Song, Jingcai Chen, Hefei Ling, "Fast Large Scale Deep Face Search," Pattern Recognition Letters, Januray 3, 2019. (SCI,EI)
%\end{thebibliography}
%\end{frame}
%------------------------------------------------
\begin{frame}
\Huge{\centerline{The End}}
\end{frame}
\end{CJK*}
\end{document}
%\end{document}
