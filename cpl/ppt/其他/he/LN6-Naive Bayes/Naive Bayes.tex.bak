%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[9pt]{beamer}
\usepackage{CJK}
\usepackage{ctex}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{longtable}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathtools}
\usepackage{animate}
%\usepackage{media9}
%% A LATEX package for embedding interactive Adobe Flash (SWF) and 3D files (Adobe U3D & PRC) as well as video and sound files or streams (FLV, MP4/H.246, MP3) into PDF documents with Adobe Reader-9/X
%compatibility.
\renewcommand{\algorithmicrequire}{\textbf{Input:}}   %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{\textbf{Output:}}  %UseOutput in the format of Algorithm
\newcommand{\e}[1]{\ensuremath{\times 10^{#1}}}
%\mode<presentation>{\usetheme{Madrid}}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\begin{document}
\begin{CJK*}{GBK}{kai}
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Machine Learning]{Bayes Classifier and Naive Bayes} % The short title appears at the bottom of every slide, the full title is only on the title page


\author{YuHua Li(李玉华)} % Your name
%\logo{%
%   \includegraphics[scale=.2]{logo.pdf}\hspace*{4.75cm}~%
%   \includegraphics[scale=.2]{logo.jpg}\hspace*{0.75cm}%
%   }
%\pgfdeclareimage[width=1cm]{hust}{logo.pdf}
%\logo{\pgfuseimage{hust}{\vspace{-10pt}}}
\titlegraphic{\includegraphics[width=1.3cm]{logo.pdf}}
\institute[IEC, HUST] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Intelligent and Embedded Computing Lab,\\
                   Huazhong University of Science \& Technology \\ % Your institution for the title page
\medskip
\textit{fuhao\_zou@hust.edu.cn} % Your email address
}

\date{2019年04月09日} % Date, can be changed to a custom date
%====================================================
\frame{\titlepage}

\frame{\frametitle{Table of contents}\tableofcontents}

\AtBeginSection[]
{
\begin{frame}{Table of Contents}
\tableofcontents[currentsection]
\end{frame}
}

%------------------------------------------------
%------------------------------------------------
\section{Basic theory}
%------------------------------------------------
\subsection{Introduction:}
\begin{frame}
\frametitle{Introduction:}
\begin{block}{Basic idea:}
	In machine learning, the naive Bayes classifier is a series of simple probability classifiers based on the Bayesian theorem under strong independent assumptions.
\end{block}

\begin{itemize}
	\item Training Data:$D$ =  \{($\mathbf{x}_1$,$y_1$),...,($\mathbf{x}_n$,$y_n$)\},($\mathbf{x}_i$,$y_i$) is sampled i.i.d from unknown distribution $P(X,Y)$.So we obtain:
	\[ P(D)=P ((\mathbf{x}_1,y_1),...,(\mathbf{x}_n,y_n)) = \prod_{\alpha=1}^{n} P(\mathbf{x}_\alpha,y_\alpha). \]
	\item Estimate $P(X,Y)$:
	\[\hat P(\mathbf{x},y)=\frac{\sum_{i=1}^{n} I(\mathbf{x}_i = x \wedge y_i = y)}{n}.\]
	\[I(\mathbf{x}_i=x \wedge y_i=y)=1\quad  if\quad  \mathbf{x}_i=x \quad and\quad y_i=y.\]
	\item Estimate $P(y|\mathbf{x})$:predict the label $y$ from the features $\mathbf{x}$
	\[\hat{P}(y|\mathbf{x}) = \frac{\hat{P}(y,\mathbf{x})}{P(\mathbf{x})} = \frac{\sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x} \wedge {y}_i = y)}{ \sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x})}. \]
	%\item Regression rule: The output is the property value for the object. This value is the average of the values of \textbf{k} nearest neighbors.	
\end{itemize}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Visualization:}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{Venn_diagram}
	%\caption{Venn diagram}
\end{figure}
\begin{block}{Venn diagram}
\begin{itemize}
\item The Venn diagram illustrates that the MLE method estimates:
\[\hat{P}(y|\mathbf{x}) = \frac{|C|}{|B|}. \]
\end{itemize}
\end{block}
\end{frame}
%------------------------------------------------
%------------------------------------------------
\section{Naive Bayes}
%------------------------------------------------
\subsection{Bayes rule}
\begin{frame}
\frametitle{Bayes rule}
If we can estimate $P$($y$) and $P$($\mathbf{x}$ $|$ $y$),since, by Bayes rule,
\[P(y | \mathbf{x}) = \frac{P(\mathbf{x} | y)P(y)}{P(\mathbf{x})}.\]

\begin{block}{Estimating $P$($y$)}
\begin{itemize}
	\item Estimating $P$($y$) is easy:For example, if $Y$ takes on discrete binary values estimating $P$($Y$) reduces to coin tossing. We simply need to count how many times we observe each outcome (in this case each class):
	\[P(y = c)  = \frac{\sum_{i=1}^{n} I(y_i = c)}{n} = \hat\pi_c\]
\end{itemize}
\end{block}

\end{frame}
%------------------------------------------------
\subsection{Naive Bayes Assumption}
\begin{frame}
Naive Bayes Assumption:
\[P(\mathbf{x} | y) = \prod_{\alpha = 1}^{d} P(x_\alpha | y), \text{where } x_\alpha = [\mathbf{x}]_\alpha \text{ is the value for feature } \alpha .\]
i.e.,feature values are independent given the label!
\frametitle{Estimating $P$($\mathbf{x}$ $|$ $y$)}
\begin{block}{Bayes Classifier}
Because of the Naive Bayes assumption
\begin{align}
	h(\mathbf{x}) &= \operatorname*{argmax}_y P(y | \mathbf{x}) \\
	&= \operatorname*{argmax}_y \; \frac{P(\mathbf{x} | y)P(y)}{P(\mathbf{x})} \\
	&= \operatorname*{argmax}_y \; P(\mathbf{x} | y) P(y) && \text{($P(\mathbf{x})$ does not depend on $y$)} \\
	&= \operatorname*{argmax}_y \; \prod_{\alpha=1}^{d} P(x_\alpha | y) P(y) && \text{(by the naive Bayes assumption)}\\
	&= \operatorname*{argmax}_y \; \sum_{\alpha = 1}^{d} \log(P(x_\alpha | y)) + \log(P(y)) && \text{(as log is a monotonic function)}
\end{align}
\end{block}
\end{frame}
%%------------------------------------------------


%%------------------------------------------------
\section{Estimating $P$([$\mathbf{x}]_\alpha$ $|$ $y$)}
%------------------------------------------------
\subsection{Categorical features}
\begin{frame}
\frametitle{Categorical features}
Features:
\[ [\mathbf{x}]_\alpha \in \{f_1, f_2, \cdots, f_{K_\alpha}\}.\]


Model $P$($x_\alpha$ $|$ $y$):
\[P(x_{\alpha} = j | y=c) = [\theta_{jc}]_{\alpha} \text{ and } \sum_{j=1}^{K_\alpha} [\theta_{jc}]_{\alpha} = 1.\]
$[\theta_{jc}]_{\alpha}$ is the probability of feature $\alpha$ having the value j, given that the label is c.And the constraint indicates that $x_{\alpha}$ must have one of the categories $\{1, \dots, K_\alpha\}$.


Parameter estimation:
\begin{align}
	[\hat\theta_{jc}]_{\alpha} &= \frac{\sum_{i=1}^{n} I(y_i = c) I(x_{i\alpha} = j) + l}{\sum_{i=1}^{n} I(y_i = c) + lK_\alpha},
\end{align}
\[ x_{i\alpha} = [\mathbf{x}_i]_\alpha,\]
$l$ is a smoothing parameter. By setting $l$=0 we get an MLE estimator, $l$ $>$ 0 leads to MAP. If we set $l$=+1 we get Laplace smoothing.
in words,this means:
\[\frac{\text{ of samples with label c that have feature } \alpha \text{ with value $j$ }}{\text{ of samples with label $c$}}.\]
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Prediction}
\[h(\mathbf{x}) = \operatorname*{argmax}_y \; \prod_{\alpha=1}^{d} P(x_\alpha | y) P(y) \]
\[\operatorname*{argmax}_y \; P(y=c \mid \mathbf{x}) \propto \operatorname*{argmax}_y \; \hat\pi_c \prod_{\alpha = 1}^{d} [\hat\theta_{jc}]_\alpha \]
\[\hat\pi_c = P(y = c)  = \frac{\sum_{i=1}^{n} I(y_i = c)}{n} \]
\end{frame}
%------------------------------------------------

%------------------------------------------------

%------------------------------------------------
\subsection{Multinomial features}
\begin{frame}
\frametitle{Multinomial features}
\begin{block}{Multinomial features}
If feature values don't represent categories (e.g. male/female) but counts we need to use a different model. E.g. in the text document categorization, feature value $x_\alpha$=$j$ means that in this particular document $\mathbf{x}$ the $\alpha^{th}$ word in my dictionary appears $j$ times.
Let us consider the example of spam filtering. Imagine the $\alpha^{th}$ word is indicative towards $spam$.Then if $x_{\alpha}$=10 means that this email is likely spam(as word $\alpha$ appears 10 times in it).And another email with $x'_{\alpha}$=20 should be even more likely to be spam (as the spammy word appears twice as often). With categorical features this is not guaranteed.
\end{block}
Features:
\begin{align}
	x_\alpha \in \{0, 1, 2, \dots, m\} \text{ and } m = \sum_{\alpha = 1}^d x_\alpha
\end{align}
Each feature $\alpha$ represents a count and $m$ is the length of the sequence. An example of this could be the count of a specific word $\alpha$ in a document of length $m$ and d is the size of the vocabulary.
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Model P($\mathbf{x}$$|$$y$)}
Use the multinomial distribution:
\[P(\mathbf{x} \mid m, y=c) = \frac{m!}{x_1! \cdot x_2! \cdot \dots \cdot x_d!} \prod_{\alpha = 1}^d\left(\theta_{\alpha c}\right)^{x_\alpha}\]
where $\theta_{\alpha c}$ is the probability of selecting $x_\alpha$ and $\sum_{\alpha = 1}^d \theta_{\alpha c} =1$ So, we can use this to generate a spam email, i.e., a document $\mathbf{x}$ of class $y = \text{spam}$ by picking m words independently at random from the vocabulary of $d$ words using $P(\mathbf{x} \mid y = \text{spam})$.
Parameter estimation:
\begin{align}
	\hat\theta_{\alpha c} = \frac{\sum_{i = 1}^{n} I(y_i = c) x_{i\alpha} + l}{\sum_{i=1}^{n} I(y_i = c) m_i + l \cdot d }
\end{align}
where $m_i=\sum_{\beta = 1}^{d} x_{i\beta}$ denotes the number of words in document $i$.The numerator sums up all counts for feature $x_\alpha$ and the denominator sums up all counts of all features across all data points.In words:
\[\frac{\text{ of times word } \alpha \text{ appears in all spam emails}}{\text{ of words in all spam emails combined}}.\]
Prediction:
\[\operatorname*{argmax}_c \; P(y = c \mid \mathbf{x}) \propto \operatorname*{argmax}_c \; \hat\pi_c \prod_{\alpha = 1}^d \hat\theta_{\alpha c}^{x_\alpha}\]
\end{frame}
%------------------------------------------------
\subsection{Continuous features (Gaussian Naive Bayes)}
\begin{frame}
\frametitle{Continuous features}
Features:
\begin{align}
	x_\alpha \in \mathbb{R} && \text{(each feature takes on a real value)}
\end{align}
Model $P(x_\alpha \mid y)$ Use Gaussian distribution:
\begin{align}
	P(x_\alpha \mid y=c) = \mathcal{N}\left(\mu_{\alpha c}, \sigma^{2}_{\alpha c}\right) = \frac{1}{\sqrt{2 \pi} \sigma_{\alpha c}} e^{-\frac{1}{2} \left(\frac{x_\alpha - \mu_{\alpha c}}{\sigma_{\alpha c}}\right)^2}
\end{align}
Note that the model specified above is based on our assumption about the data - that each feature $\alpha$ comes from a class-conditional Gaussian distribution.
The full distribution:
\[P(\mathbf{x}|y)\sim \mathcal{N}(\mathbf{\mu}_y,\Sigma_y) \]
where $\Sigma_y$  is a diagonal covariance matrix with
\[ [\Sigma_y]_{\alpha,\alpha}=\sigma^2_{\alpha,y} \]

\end{frame}
%------------------------------------------------
%------------------------------------------------
\begin{frame}
\frametitle{Parameter estimation:}
\begin{block}{Parameter estimation:}
As always, we estimate the parameters of the distributions for each dimension and class independently. Gaussian distributions only have two parameters, the mean and variance. The mean $\mu_{\alpha,y}$,$y$ is estimated by the average feature value of dimension $\alpha$ from all samples with label $y$. The (squared) standard deviation is simply the variance of this estimate.
\end{block}
\begin{align}
	\mu_{\alpha c} &\leftarrow \frac{1}{n_c} \sum_{i = 1}^{n} I(y_i = c) x_{i\alpha} && \text{where $n_c = \sum_{i=1}^{n} I(y_i = c)$} \\
	\sigma_{\alpha c}^2 &\leftarrow \frac{1}{n_c} \sum_{i=1}^{n} I(y_i = c)(x_{i\alpha} - \mu_{\alpha c})^2
\end{align}
\end{frame}
%------------------------------------------------
%%------------------------------------------------
\section{Naive Bayes classifier}
%------------------------------------------------
\subsection{Naive Bayes is a linear classifier}
\begin{frame}
\frametitle{Multinomial Features}
Suppose that $y_i \in \{-1, +1\}$ and features are multinomial.So:
\[h(\mathbf{x}) = \operatorname*{argmax}_y \; P(y) \prod_{\alpha - 1}^d P(x_\alpha \mid y) = \textrm{sign}(\mathbf{w}^\top \mathbf{x} + b) \]
\[\mathbf{w}^\top \mathbf{x} + b > 0 \Longleftrightarrow h(\mathbf{x}) = +1.\]
As before, we define:
\[P(x_\alpha|y=+1)\propto\theta_{\alpha+}^{x_\alpha};P(Y=+1)=\pi_+.\]
\begin{align}
	[\mathbf{w}]_\alpha &= \log(\theta_{\alpha +}) - \log(\theta_{\alpha -}) \\
	b &= \log(\pi_+) - \log(\pi_-)
\end{align}

\end{frame}
%------------------------------------------------
\begin{frame}
\begin{align}
		\mathbf{w}^\top \mathbf{x} + b > 0 &\Longleftrightarrow \sum_{\alpha = 1}^{d} [\mathbf{x}]_\alpha
		\overbrace{(\log(\theta_{\alpha +}) - \log(\theta_{\alpha -}))}^{[\mathbf{w}]_\alpha} + \overbrace{\log(\pi_+) - \log(\pi_-)}^b > 0  \\
		&\Longleftrightarrow \exp\left(\sum_{\alpha = 1}^{d} [\mathbf{x}]_\alpha
		{(\log(\theta_{\alpha +}) - \log(\theta_{\alpha -}))} + {\log(\pi_+) - \log(\pi_-)} \right)> 1  \\
		&\Longleftrightarrow \prod_{\alpha = 1}^{d}
		\frac{\exp\left( \log\theta_{\alpha +}^{[\mathbf{x}]_\alpha} + \log(\pi_+)\right)}
		{\exp\left(\log\theta_{\alpha -}^{[\mathbf{x}]_\alpha} + \log(\pi_-)\right)}
		> 1   \\
		&\Longleftrightarrow \prod_{\alpha = 1}^{d}
		\frac{\theta_{\alpha +}^{[\mathbf{x}]_\alpha} \pi_+}
		{\theta_{\alpha -}^{[\mathbf{x}]_\alpha} \pi_-} > 1   \\
		&\Longleftrightarrow \frac{\prod_{\alpha = 1}^{d} P([\mathbf{x}]_\alpha | Y = +1)\pi_+}{\prod_{\alpha =1}^{d}P([\mathbf{x}]_\alpha | Y = -1)\pi_-} > 1 \\
		&\Longleftrightarrow \frac{P(\mathbf{x} | Y = +1)\pi_+}{P(\mathbf{x} | Y = -1)\pi_-} > 1 \\
		&\Longleftrightarrow \frac{P(Y = +1 |\mathbf{x})}{P( Y = -1|\mathbf{x})}>1  \\
		&\Longleftrightarrow P(Y = +1 | \mathbf{x}) > P(Y = -1 | \mathbf{x})  \\
		&\Longleftrightarrow \operatorname*{argmax}_y  P(Y=y|\mathbf{x})=+1
\end{align}
\end{frame}
%------------------------------------------------

%------------------------------------------------
\subsection{Gaussian Naive Bayes}
\begin{frame}
\frametitle{Gaussian Naive Bayes}
\begin{block}{Gaussian Naive Bayes}
In the case of continuous features (Gaussian Naive Bayes), we can show that:
\[P(y \mid \mathbf{x}) = \frac{1}{1 + e^{-y (\mathbf{w}^\top \mathbf{x} +b) }} \]
This model is also known as logistic regression.
\end{block}
\end{frame}

%%------------------------------------------------
\section{Examples and Application}
%------------------------------------------------
\subsection{Filter spam with naive bayes}
\begin{frame}
\frametitle{Filter spam with naive bayes}
\begin{block}{Core algorithm:Naive Bayesian classifier training function}
	def trainNB0(trainMatrix, trainCategory):计算训练的文档数目\\
	\quad numTrainDocs = len(trainMatrix)计算文档的词条数\\
	\quad numWords = len(trainMatrix[0])文档属于侮辱类的概率\\
	\quad pAbusive = sum(trainCategory)/float(numTrainDocs)初始化\\
    \quad p0Num = ones(numWords); p1Num = ones(numWords) \\
    \quad p0Denom = 2.0; p1Denom = 2.0\\
    \quad for i in range(numTrainDocs):\\
	\qquad if trainCategory[i] == 1:统计计算词语属于侮辱类的条件概率所需的数据\\
	\qquad \quad p1Num += trainMatrix[i]\\
	\qquad \quad p1Denom += sum(trainMatrix[i])\\
	\qquad else:统计计算属于非侮辱类的条件概率所需的数据\\
	\qquad \quad p0Num += trainMatrix[i]\\
	\qquad \quad p0Denom += sum(trainMatrix[i])\\
	\quad 相除计算概率向量\\
    \quad p1Vect = log(p1Num / p1Denom)\\
	\quad p0Vect = log(p0Num / p0Denom)\\
	\quad 返回词语属于侮辱类的条件概率向量，词语属于非侮辱类的条件概率向量，文档属于侮辱类的概率\\
	\quad return p0Vect, p1Vect, pAbusive
\end{block}
\end{frame}
%------------------------------------------------
\begin{frame}
\frametitle{Classify}
\begin{block}{Classify}
	def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\\
	\quad 输入为需要分类的词向量，以及词语属于侮辱类的条件概率向量，词语属于非侮辱类的条件概率向量，文档属于侮辱类的概率\\
    \quad p1=sum(vec2Classify*p1Vec)+log(pClass1)\\
    \quad p0=sum(vec2Classify*p0Vec)+log(1.0-pClass1)\\
    \quad if p1 $>$ p0:\\
        \qquad \quad return 1\\
    \quad else:\\
		\qquad \quad return 0\\
\end{block}
Because:
\[p(c_{i} | \textbf{w}) = \frac{p(\textbf{w} | c_{i})p(c_{i})}{p(\textbf{w})},w:word \ vector;c_{i}:label\]
\[p(\textbf{w} | c_{i}) = p(w_{0},w_{1},...,w_{N} | c_{i}) = p(w_{0} | c_{i})p(w_{1} | c_{i})...p(w_{N} | c_{i})\]
\[log(p(\textbf{w} | c_{i})p(c_{i})) \]
\[= log(p(w_{0} | c_{i})p(w_{1} | c_{i})...p(w_{N} | c_{i})p(c_{i}))  \]
\[= log(p(w_{0} | c_{i})) + log(p(w_{1} | c_{i})) + ... +log(p(w_{N} | c_{i})) + log(p(c_{i})) \]
\end{frame}
%%------------------------------------------------
\section{Naive Bayes summary}
\subsection{Summary of Naive Bayes}
\begin{frame}
\frametitle{Summary of Naive Bayes}
Bayesian formula:
\[p(X|Y) = \frac{p(Y|X)p(X)}{p(Y)}\]
Assumption:
\[p(x_{1},x_{2},...,x_{n} | y ) = p(x_{1} | y)p(x_{2} | y)...p(x_{n} | y)\]
Likelihood function:
\[ \prod_{i=1}^{n}p(x_{i} | y,\theta) \]
Log-likelihood:
\[\sum_{i=1}^{n}log(p(x_{i} | y,\theta)) \]
Maximum likelihood estimation:
\[\operatorname*{argmax}_{\theta} \sum_{i=1}^{n}log(p(x_{i} | y,\theta)) \]
Classify:
\[ \operatorname*{argmax}_{y} p(y)\prod_{i=1}^{n}p(x_{i} | y,\theta) = \operatorname*{argmax}_{y} log(p(y)) + \sum_{i=1}^{n}log(p(x_{i} | y,\theta)) \]

\end{frame}
%------------------------------------------------

%%------------------------------------------------
%\section{Reference}
%------------------------------------------------
%\begin{frame}
%\frametitle{Reference}
%\begin{thebibliography}{4}
%\bibitem{LS-SPH} F. Zou, C. Liu, H. Ling, H. Feng, L. Yan, and D. Li, "Least square regularized spectral hashing for similarity search," Signal Processing, vol. 93, pp. 2265-2273, 2013. (SCI,EI)
%\bibitem{KMFH} F. Zou, Y. Chen, J. Song, K. Zhou, Y. Yang, and N. Sebe, "Compact image fingerprint via multiple kernel hashing," IEEE Transactions on Multimedia, vol. 17, pp. 1006-1018, 2015. (SCI,EI)
%\bibitem{KNPH} C. Liu, H. Ling, F. Zou, L. Yan, Y. Wang, H. Feng, et al., "Kernelized neighborhood preserving hashing for social-network-oriented digital fingerprints," IEEE Transactions on Information Forensics and Security, vol. 9, pp. 2232-2247, 2014. (SCI,EI)
%\bibitem{DTSH} Liu, Yu; Song, Jingkuan; Zhou, Ke; Yan, Lingyu; Liu, Li; Zou, Fuhao; Shao, Ling, "Deep Self-taught Hashing for Image Retrieval," IEEE Transactions on Cybernetics, May 3, 2018. (SCI,EI)
%\bibitem{DeepFace} Fuhao Zou, Fan Yang, Wei Chen,Kai Lia, Jingkuan Song, Jingcai Chen, Hefei Ling, "Fast Large Scale Deep Face Search," Pattern Recognition Letters, Januray 3, 2019. (SCI,EI)
%\end{thebibliography}
%\end{frame}
%------------------------------------------------
\begin{frame}
\Huge{\centerline{The End}}
\end{frame}
\end{CJK*}
\end{document}
%\end{document}
