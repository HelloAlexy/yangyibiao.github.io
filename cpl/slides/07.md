---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 神经网络

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 大纲

---

@import "../vega/outline.json" {as="vega" .top-2}

<!-- slide vertical=true data-notes="" -->

##### 发展历史

---

@import "../mermaid/nn.mermaid"

<div class="top-2"></div>

- 八十年代红极一时：x86 系列 CPU 和内存条技术较七十年代显著提高
- 近十年梅开二度：大数据防止过拟合，显卡等计算设备性能显著提升

<!-- slide data-notes="" -->

##### 神经网络

---

@import "../dot/nn.dot" {.center}

<div class="top0"></div>

- 黄色部分就是个 M-P 神经元模型
- 大量的神经元并行串联就构成了神经网络
- 只要存在隐藏层，神经网络就拥有了非线性分类能力

<!-- slide vertical=true data-notes="" -->

##### 形式化

---

引入下面的记号：

- $L$：神经网络的层数
- $n_l$：第$l$层神经元的个数
- $h_l(\cdot)$：第$l$层的激活函数
- $\Wv_l \in \Rbb^{n_l \times n_{l-1}}$：第$l-1$层到第$l$层的权重矩阵
- $\bv_l \in \Rbb^{n_l}$：第$l$层的偏置 (截距)
- $\zv_l \in \Rbb^{n_l}$：第$l$层神经元的输入
- $\av_l \in \Rbb^{n_l}$：第$l$层神经元的输出

<div class="top4"></div>

神经网络第$l$层的计算过程：$\zv_l = \Wv_l \av_{l-1} + \bv_l$，$\av_l = h_l (\zv_l)$

整个网络：$\xv = \av_0 \xrightarrow{\Wv_1,\bv_1} \zv_1 \xrightarrow{h_1} \av_1 \xrightarrow{\Wv_2,\bv_2} \cdots \xrightarrow{\Wv_L,\bv_L} \zv_L \xrightarrow{h_L} \av_L = \hat{\yv}$

<!-- slide data-notes="" -->

##### 激活函数

---

最早的 M-P 模型采用阶跃函数$\sgn(\cdot)$作为激活函数

改进方向：

- 连续并几乎处处可导，可以高效计算
- 导数的值域在合适的范围内，否则影响用梯度下降进行训练

<div class="top2"></div>

常见的有

- Sigmoid 型：对率函数，双曲正切函数
- ReLU，带泄漏的 ReLU，带参数的 ReLU，ELU，Softplus
- Swish 函数
- Maxout 单元

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">Sigmoid</span> 型

---

@import "../python/sigmoid.svg" {.width70 .center .top6}

<!-- slide vertical=true data-notes="" -->

##### 对率函数

---

将$\Rbb${==挤压==}到$[0,1]$，输出拥有{==概率==}意义：

$$
\begin{align*}
    \quad \sigma(z) = \frac{1}{1 + \exp (-z)} = \begin{cases}
        1, & z \rightarrow \infty \\
        0, & z \rightarrow -\infty
    \end{cases}
\end{align*}
$$

对率函数连续可导，在{==零处导数最大==}

$$
\begin{align*}
    \quad \nabla \sigma(z) = \sigma(z) (1 - \sigma(z)) \le \left( \frac{\sigma(z) + 1 - \sigma(z)}{2} \right)^2 = \frac{1}{4}
\end{align*}
$$

<div class="top-3"></div>

均值不等式等号成立的条件是$\sigma(z) = 1 - \sigma(z)$，即$z = 0$

<!-- slide vertical=true data-notes="" -->

##### 双曲正切函数

---

将$\Rbb${==挤压==}到$[-1,1]$，{==输出零中心化==}，对率函数的放大平移

$$
\begin{align*}
    \quad \tanh(z) & = \frac{\exp(z) - \exp(-z)}{\exp(z) + \exp(-z)} = \frac{1 - \exp(-2z)}{1 + \exp(-2z)} = 2 \sigma(2z) - 1 \\[2pt]
    & = \begin{cases}
        1, & z \rightarrow \infty \\
        -1, & z \rightarrow -\infty
    \end{cases} \\[10pt]
    \nabla \tanh(z) & = 4 \sigma(2z) (1 - \sigma(2z)) \le 1
\end{align*}
$$

双曲正切函数连续可导，在$z = 0$处导数最大

输出零中心化使得非输入层的输入都在零附近，而双曲正切函数在零处导数最大，梯度下降更新效率较高，对率函数输出恒为正，会减慢梯度下降的收敛速度

<!-- slide data-notes="" -->

##### 整流线性单元

---

整流线性单元 (<u>re</u>ctified <u>l</u>inear <u>u</u>nit, ReLU)：

$$
\begin{align*}
    \quad \relu(z) = \max \{ 0, z \} = \begin{cases}
        z & z \ge 0 \\ 0 & z < 0
    \end{cases}
\end{align*}
$$

<div class="top-2"></div>

优点

- 计算只涉及加法、乘法和比较操作，非常高效
- 生物学解释：单侧抑制，宽兴奋边界，稀疏兴奋
- 在$z > 0$时导数恒为$1$，缓解了{==梯度消失==}问题

<div class="top2"></div>

缺点

- 输出非零中心化，对下一层不友好
- 死亡 ReLU 问题：对异常值特别敏感

<!-- slide vertical=true data-notes="" -->

##### 死亡 <span style="font-weight:900">ReLU</span> 问题

---

由链式法则有

$$
\begin{align*}
    \quad \nabla_{\wv} \relu(\wv^\top \xv + b) & = \frac{\partial \relu(\wv^\top \xv + b)}{\partial (\wv^\top \xv + b)} \frac{\partial (\wv^\top \xv + b)}{\partial \wv} \\
    & = \frac{\partial \max \{ 0, \wv^\top \xv + b \}}{\partial (\wv^\top \xv + b)} \xv \\
    & = \Ibb(\wv^\top \xv + b \ge 0) \xv
\end{align*}
$$

如果第一个隐藏层中的某个神经元对应的$(\wv,b)$初始化不当，使得对任意$\xv$有$\wv^\top \xv + b < 0$，那么其关于$(\wv,b)$的梯度将为零，在以后的训练过程中永远不会被更新

解决方案：带泄漏的 ReLU，带参数的 ReLU，ELU，Softplus

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">ReLU</span> 变体

---

带泄漏的 ReLU：当$\wv^\top \xv + b < 0$时也有非零梯度

$$
\begin{align*}
    \quad \lrelu(z) & = \begin{cases}
        z & z \ge 0 \\ \gamma z & z < 0
    \end{cases} \\
    & = \max \{ 0, z \} + \gamma \min \{ 0, z \} \overset{\gamma < 1}{=} \max \{ z, \gamma z \}
\end{align*}
$$

<div class="top-3"></div>

其中斜率$\gamma$是一个很小的常数，比如$0.01$

带参数的 ReLU：斜率$\gamma_i$可学习

$$
\begin{align*}
    \quad \prelu(z) & = \begin{cases}
        z & z \ge 0 \\ \gamma_i z & z < 0
    \end{cases} \\[4pt]
    & = \max \{ 0, z \} + \gamma_i \min \{ 0, z \}
\end{align*}
$$

<div class="top-3"></div>

可以不同神经元有不同的参数，也可以一组神经元共享一个参数

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">ReLU</span> 变体

---

指数线性单元 (<u>e</u>xponential <u>l</u>inear <u>u</u>nit, ELU)

$$
\begin{align*}
    \quad \elu(z) & = \begin{cases}
        z & z \ge 0 \\ \gamma (\exp(z) - 1) & z < 0
    \end{cases} \\[4pt]
    & = \max \{ 0, z \} + \min \{ 0, \gamma (\exp(z) - 1) \}
\end{align*}
$$

Softplus 函数可以看作 ReLU 的平滑版本：

$$
\begin{align*}
    \quad \softplus(z) = \ln (1 + \exp(z))
\end{align*}
$$

其导数为对率函数

$$
\begin{align*}
    \quad \nabla \softplus(z) = \frac{\exp(z)}{1 + \exp(z)} = \frac{1}{1 + \exp(-z)}
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">ReLU</span> 族

---

@import "../python/relu.svg" {.width70 .center .top6}

<!-- slide data-notes="自门控的意思是控制自己是否激活的\sigma (\beta z)也跟有关" -->

##### <span style="font-weight:900">Swish</span> 函数

---

Swish 函数是一种自门控 (self-gated) 激活函数：

$$
\begin{align*}
    \quad \swish(z) = z \cdot \sigma (\beta z) = \frac{z}{1 + \exp(-\beta z)}
\end{align*}
$$

<div class="top-4"></div>

其中$\beta$是可学习的参数或一个固定超参数

- 当$\sigma (\beta z)$接近于$1$时，门处于{==开==}状态，激活函数的输出近似于$z$本身
- 当$\sigma (\beta z)$接近于$0$时，门处于{==关==}状态，激活函数的输出近似于$0$

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">Swish</span> 函数

---

@import "../python/swish.svg" {.width70 .center .top6}

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">Maxout</span> 单元

---

考虑神经网络的第$l$层：

$$
\begin{align*}
    \quad \zv_l & = \Wv_l \av_{l-1} + \bv_l \\
    \av_l & = h_l (\zv_l)
\end{align*}
$$

<div class="top-4"></div>

前面提到的激活函数都是$\Rbb \mapsto \Rbb$的，即$[\av_l]_i = h_l ([\zv_l]_i), ~ i \in [n_l]$

Maxout 单元是$\Rbb^{n_l} \mapsto \Rbb$的，输入就是$\zv_l$，其定义为

$$
\begin{align*}
    \quad \maxout (\zv) = \max_{k \in [K]} \{ \wv_k^\top \zv + b_k \}
\end{align*}
$$

- 整体学习输入到输出间的非线性关系
- $\relu(z) = \max \{ 0, z \}$与$\lrelu(z) \overset{\gamma < 1}{=} \max \{ z, \gamma z \}$都是 Maxout 单元的特例

<!-- slide data-notes="" -->

##### 应用到机器学习

---

@import "../dot/ml-nn.dot"

<div class="top-2"></div>

前$L-1$层是复合函数$\psi: \Rbb^d \mapsto \Rbb^{n_{L-1}}$，可看作一种特征变换方法

最后一层是学习器$\hat{\yv} = g(\psi(\xv); \Wv_L, \bv_L)$，对输入进行预测

- 若$y \in \{ \pm 1 \} \text{ or } \{ 1,0 \}$，最后一层只需$1$个神经元，采用对率激活函数
- 若$y \in [c]$，最后一层需$c$个神经元，采用 Softmax 激活函数

<div class="top2"></div>

我的批注 对率回归也可看作只有一层(没有隐藏层)的神经网络

<!-- slide vertical=true data-notes="" -->

##### 深度学习

---

传统机器学习：特征工程和模型学习两阶段分开进行

@import "../dot/ml-old.dot"

<div class="top2"></div>

深度学习：特征工程和模型学习合二为一，端到端 (end-to-end)

@import "../dot/ml-nn.dot"

<!-- slide data-notes="" -->

##### 求解参数

---

整个网络：$\xv = \av_0 \xrightarrow{\Wv_1,\bv_1} \zv_1 \xrightarrow{h_1} \av_1 \xrightarrow{\Wv_2,\bv_2} \cdots \xrightarrow{\Wv_L,\bv_L} \zv_L \xrightarrow{h_L} \av_L = \hat{\yv}$

神经网络的优化目标为

$$
\begin{align*}
    \quad \min_{\Wv, \bv} ~ \frac{1}{m} \sum_{i \in [m]} \ell (\yv_i, \hat{\yv}_i)
\end{align*}
$$

<div class="top-5"></div>

其中损失$\ell (\yv, \hat{\yv})$的计算为{==正向传播==}

- 样本从输入层进入，经隐藏层逐层传播到最后输出层
- $\hat{\yv} = \av_L = h_L (\zv_L)$是对样本$\xv$的预测，据此计算$\ell (\yv, \hat{\yv}) = \ell (\yv, h_L (\zv_L))$

<div class="top4"></div>

梯度下降更新公式为

$$
\begin{align*}
    \quad \Wv ~ \leftarrow ~ \Wv - \frac{\eta}{m} \sum_{i \in [m]} \class{yellow}{\frac{\partial \ell (\yv_i, \hat{\yv}_i)}{\partial \Wv}}, \quad \bv ~ \leftarrow ~ \bv - \frac{\eta}{m} \sum_{i \in [m]} \class{yellow}{\frac{\partial \ell (\yv_i, \hat{\yv}_i)}{\partial \bv}}
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 求解参数

---

整个网络：$\xv = \av_0 \xrightarrow{\Wv_1,\bv_1} \zv_1 \xrightarrow{h_1} \av_1 \xrightarrow{\Wv_2,\bv_2} \cdots \xrightarrow{\Wv_L,\bv_L} \zv_L \xrightarrow{h_L} \av_L = \hat{\yv}$

最后一层$\zv_L = \Wv_L ~ \av_{L-1} + \bv_L$，$\av_L = h_L (\zv_L)$，由{==链式法则==}有

$$
\begin{align*}
    \quad \frac{\partial \ell (\yv, \hat{\yv})}{\partial \bv_L} & = \frac{\partial \ell (\yv, \hat{\yv})}{\partial \zv_L} \frac{\partial \zv_L}{\partial \bv_L} = \deltav_L^\top \frac{\partial \zv_L}{\partial \bv_L} = \deltav_L^\top \\
    \frac{\partial \ell (\yv, \hat{\yv})}{\partial \Wv_L} & = \sum_{j \in [n_L]} \frac{\partial \ell (\yv, \hat{\yv})}{\partial [\zv_L]_j} \frac{\partial [\zv_L]_j}{\partial \Wv_L} = \sum_{j \in [n_L]} [\deltav_L]_j \frac{\partial [\zv_L]_j}{\partial \Wv_L}
\end{align*}
$$

<div class="top-5"></div>

其中$\deltav_L^\top = \partial \ell (\yv, \hat{\yv}) / \partial \zv_L \in \Rbb^{n_L}$为第$L$层的{==误差项==}，可直接求解

<div class="top-2"></div>

类似的，对第$l$层$\zv_l = \Wv_l \av_{l-1} + \bv_l$，$\av_l = h_l (\zv_l)$，由{==链式法则==}有

$$
\begin{align*}
    \quad \frac{\partial \ell (\yv, \hat{\yv})}{\partial \bv_l} = \deltav_l^\top, \quad \frac{\partial \ell (\yv, \hat{\yv})}{\partial \Wv_l} = \sum_{j \in [n_l]} [\deltav_l]_j \frac{\partial [\zv_l]_j}{\partial \Wv_l}
\end{align*}
$$

<div class="top-5"></div>

其中$\deltav_l^\top = \partial \ell (\yv, \hat{\yv}) / \partial \zv_l \in \Rbb^{n_l}$为第$l$层的{==误差项==}

<!-- slide data-notes="" -->

##### 反向传播

---

{==反向传播==} (<u>b</u>ack<u>p</u>ropagation, BP)：前一层误差由后一层得到

$$
\begin{align*}
    \quad \deltav_{l-1}^\top = \frac{\partial \ell (\yv, \hat{\yv})}{\partial \zv_{l-1}} = \frac{\partial \ell (\yv, \hat{\yv})}{\partial \zv_l} \frac{\partial \zv_l}{\partial \av_{l-1}} \frac{\partial \av_{l-1}}{\partial \zv_{l-1}} = \deltav_l^\top \Wv_l \frac{\partial h_{l-1}(\zv_{l-1})}{\partial \zv_{l-1}}
\end{align*}
$$

最后对第$l$层$\zv_l = \Wv_l \av_{l-1} + \bv_l$，如何求$\partial [\zv_l]_j / \partial \Wv_l$？

注意$z_j = \sum_k w_{jk} a_k + b_k$只与$\Wv$的第$j$行有关，于是

$$
\begin{align*}
    \quad & \frac{\partial z_j}{\partial \Wv} = \underbrace{\begin{bmatrix} \zerov, \ldots, \av, \ldots, \zerov \end{bmatrix}}_{\text{only }\av\text{ at }j\text{-th column}} = \av \ev_j^\top \\[4pt]
    \quad & \Longrightarrow \frac{\partial \ell (\yv, \hat{\yv})}{\partial \Wv_l} = \sum_{j \in [n_l]} [\deltav_l]_j \frac{\partial [\zv_l]_j}{\partial \Wv_l} = \av_{l-1} \sum_{j \in [n_l]} [\deltav_l]_j \ev_j^\top = \av_{l-1} \deltav_l^\top
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 反向传播算法

---

输入：训练集，验证集，相关超参数

1. 随机初始化$\Wv$和$\bv$
2. {==while==} 神经网络在验证集上的精度仍在上升
3. &emsp;&emsp;对训练集中的样本随机重排序
4. &emsp;&emsp;{==for==} $i = 1, \ldots, m$ {==do==}
5. &emsp;&emsp;&emsp;&emsp;获取样本$(\xv_i, \yv_i)$
6. &emsp;&emsp;&emsp;&emsp;正向传播，依次计算$\av_l = h_l(\Wv_l \av_{l-1} + \bv_l)$，最后得到$\ell (\yv_i, \hat{\yv}_i)$
7. &emsp;&emsp;&emsp;&emsp;反向传播，依次计算误差项$\deltav_l^\top = \deltav_{l+1}^\top \Wv_{l+1} \diag (h_l'(\zv_l))$
8. &emsp;&emsp;&emsp;&emsp;计算梯度$\partial \ell (\yv_i, \hat{\yv}_i) / \partial \Wv_l = \av_{l-1} \deltav_l^\top$、$\partial \ell (\yv_i, \hat{\yv}_i) / \partial \bv_l = \deltav_l^\top$
9. &emsp;&emsp;&emsp;&emsp;采用梯度下降更新$\Wv_l$和$\bv_l$

输出：$\Wv$和$\bv$

<!-- slide data-notes="" -->

##### <span style="font-weight:900">sklearn</span> 中的神经网络

---

```python {.line-numbers .top-1 .left4 highlight=[2,4-20]}
import numpy as np
from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(
    hidden_layer_sizes=(h),    # 隐藏层神经元个数
    activation='logistic',     # identity, logistic, tanh, relu
    max_iter=100,              # 最大迭代轮数
    solver='lbfgs',            # 求解器
    alpha=0,                   # 正则项系数
    batch_size=32,             # 批量大小
    learning_rate='constant',  # constant, invscaling, adaptive
    shuffle=True,              # 每轮是否将样本重新排序,
    momentum=0.9,              # 动量法系数, sgd only
    nesterovs_momentum=True,   # 动量法用Nesterov加速
    early_stopping=False,      # 是否提早停止
    warm_start=False,          # 是否开启热启动机制
    random_state=1,
    verbose=False
    ...
)

clf = mlp.fit(X, y)
acc = clf.score(X, y)
```

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">sklearn</span> 中的神经网络

---

- 以异或 4 个点为中心，从 2 维高斯分布中各采样 255 个样本
- 单隐藏层，对率激活函数，lbfgs 求解器

@import "../python/mlp-xor-neuron.svg" {.center .width92}

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">sklearn</span> 中的神经网络

---

- 以异或 4 个点为中心，从 2 维高斯分布中各采样 255 个样本
- 单隐藏层，3 个神经元，lbfgs 求解器

@import "../python/mlp-xor-activation.svg" {.center .width92}

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">sklearn</span> 中的神经网络

---

- 以异或 4 个点为中心，从 2 维高斯分布中各采样 255 个样本
- 单隐藏层，7 个神经元，ReLU 激活函数

@import "../python/mlp-xor-solver.svg" {.center .width92}

<!-- slide data-notes="" -->

##### 用 <span style="font-weight:900">TensorFlow</span> 实现

---

```python {.line-numbers .top-1 .left4 highlight=[10-21,30-49,52]}
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

model = Sequential()
model.add(Dense(units=3, activation="sigmoid", input_shape=(2, )))
model.add(Dense(units=1, activation='sigmoid'))

model.summary()  # 打印模型
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense (Dense)               (None, 3)                 9

 dense_1 (Dense)             (None, 1)                 4

=================================================================
Total params: 13
Trainable params: 13
Non-trainable params: 0
_________________________________________________________________

model.compile(
    optimizer=Adam(0.1),
    loss="binary_crossentropy",
    metrics=['accuracy']
)

model.fit(X, y, epochs=10, batch_size=32)
Epoch 1/10
32/32 [==============] - 0s 1ms/step - loss: 0.6481 - accuracy: 0.6309
Epoch 2/10
32/32 [==============] - 0s 1ms/step - loss: 0.5064 - accuracy: 0.7500
Epoch 3/10
32/32 [==============] - 0s 1000us/step - loss: 0.3309 - accuracy: 0.8369
Epoch 4/10
32/32 [==============] - 0s 1ms/step - loss: 0.1383 - accuracy: 1.0000
Epoch 5/10
32/32 [==============] - 0s 1ms/step - loss: 0.0643 - accuracy: 1.0000
Epoch 6/10
32/32 [==============] - 0s 1ms/step - loss: 0.0395 - accuracy: 1.0000
Epoch 7/10
32/32 [==============] - 0s 1ms/step - loss: 0.0276 - accuracy: 1.0000
Epoch 8/10
32/32 [==============] - 0s 1ms/step - loss: 0.0208 - accuracy: 1.0000
Epoch 9/10
32/32 [==============] - 0s 994us/step - loss: 0.0165 - accuracy: 1.0000
Epoch 10/10
32/32 [==============] - 0s 997us/step - loss: 0.0134 - accuracy: 1.0000

loss, acc = model.evaluate(X, y, verbose=2)
32/32 - 0s - loss: 0.0121 - accuracy: 1.0000 - 93ms/epoch - 3ms/step
```

<!-- slide vertical=true data-notes="" -->

##### 用 <span style="font-weight:900">TensorFlow</span> 实现

---

- 以异或 4 个点为中心，从 2 维高斯分布中各采样 255 个样本
- 单隐藏层，对率激活函数，Adam 求解器

@import "../python/dnn-xor.svg" {.center .width92}

<!-- slide data-notes="" -->

##### 梯度消失

---

神经网络中误差反向传播的迭代公式为

$$
\begin{align*}
    \quad \deltav_l^\top = \frac{\partial \ell (\yv, \hat{\yv})}{\partial \zv_l} = \frac{\partial \ell (\yv, \hat{\yv})}{\partial \zv_{l+1}} \frac{\partial \zv_{l+1}}{\partial \av_l} \frac{\partial \av_l}{\partial \zv_l} = \deltav_{l+1}^\top \Wv_{l+1} \diag (h_l'(\zv_l))
\end{align*}
$$

对于 Sigmoid 型激活函数

- $\nabla \sigma(z) = \sigma(z) (1 - \sigma(z)) \le 1/4$
- $\nabla \tanh(z) = 4 \sigma(2z) (1 - \sigma(2z)) \le 1$

<div class="top2"></div>

误差每传播一层都会乘以一个小于等于$1$的系数，当网络层数很深时，梯度会不断衰减甚至消失，使得整个网络很难训练

解决方案：使用导数比较大的激活函数，比如 ReLU

<!-- slide vertical=true data-notes="" -->

##### 残差网络

---

@import "../tikz/resnet.svg" {.width75 .center .top2 .bottom2}

残差模块 $\zv_l = \av_{l-1} + \class{yellow}{\Uv_2 \cdot h(\Uv_1 \cdot \av_{l-1} + \cv_1) + \cv_2} = \av_{l-1} + \class{yellow}{f(\av_{l-1})}$

假设$\av_l = \zv_l$，即残差模块输出不使用激活函数，对$\forall t \in [l]$有

$$
\begin{align*}
    \quad \av_l = \av_{l-1} + f(\av_{l-1}) = \av_{l-2} + f(\av_{l-2}) + f(\av_{l-1}) = \cdots = \av_{l-t} + \sum_{i=l-t}^{l-1} f(\av_i)
\end{align*}
$$

<div class="top-4"></div>

我的批注 低层输入可以{==恒等==}传播到任意高层

<!-- slide vertical=true data-notes="" -->

##### 残差网络

---

低层输入可以{==恒等==}传播到任意高层

$$
\begin{align*}
    \quad \av_l = \av_{l-t} + \sum_{i=l-t}^{l-1} f(\av_i)
\end{align*}
$$

<div class="top-4"></div>

由链式法则有

$$
\begin{align*}
    \quad \frac{\partial \ell}{\partial \av_{l-t}} & = \frac{\partial \ell}{\partial \av_l} \frac{\partial \av_l}{\partial \av_{l-t}} = \frac{\partial \ell}{\partial \av_l} \left( \frac{\partial \av_{l-t}}{\partial \av_{l-t}} + \frac{\partial }{\partial \av_{l-t}} \sum_{i=l-t}^{l-1} f(\av_i) \right) \\
    & = \frac{\partial \ell}{\partial \av_l} \left( \Iv + \frac{\partial }{\partial \av_{l-t}} \sum_{i=l-t}^{l-1} f(\av_i) \right) \\
    & = \frac{\partial \ell}{\partial \av_l} + \frac{\partial \ell}{\partial \av_l} \left( \frac{\partial }{\partial \av_{l-t}} \sum_{i=l-t}^{l-1} f(\av_i) \right)
\end{align*}
$$

<div class="top-4"></div>

我的批注 高层误差可以{==恒等==}传播到任意低层，梯度消失得以缓解

<!-- slide data-notes="" -->

##### 神经网络的变种

---

神经网络已被扩展到多种类型的数据上

@import "../dot/grid-sequence.dot" {class="left10 top2 bottom1"}

@import "../dot/graph.dot" {engine="neato" class="left64per top-24per"}

- 网格数据，如图片，卷积神经网络
- 序列数据，如文本，循环神经网络
- 图数据，如药物分子，图神经网络

<!-- slide data-notes="灵材：可免费获取的 MNIST 有 10 类，ImageNet 则有上千类，丹师是从药童做起，多模态：混合灵草和妖兽 <br><br> 丹方里最重要的是灵阵，控制如何抽取和凝结灵材中的灵性。灵阵中有若干节点，然后通过回路连接这些节点。灵材沿着回路游走经过每个节点处进行一步一步的提纯 <br><br> 半自动 不用你手动求导 做反向传播 更高端的可以使用多个丹炉同时开火炼制一枚灵丹 tf boy pt boy <br><br> 手中富裕的买 囊中羞涩的租" -->

##### 当代炼丹术

---

@import "../dot/alchemy.dot"

一个优秀丹师的自我修养：

- 灵材品质差要会手动增强，旋转、翻转、缩放、平移、加噪声
- 因材制宜设计灵阵，空间灵材用卷积灵阵，时间灵材用循环灵阵，...
- 仔细观察丹炉状态，防止爆炉，若仙丹成色不好则改进配置重新来过
