---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 对数几率回归

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 大纲

---

@import "../vega/outline.json" {as="vega" .top-2}

<!-- slide data-notes="" -->

##### 对数几率函数

---

感知机采用符号函数作为激活函数

$$
\begin{align*}
    \quad y = \sign(\wv^\top \xv) = \begin{cases} +1, & \wv^\top \xv \ge 0 \\ -1, & \wv^\top \xv < 0 \end{cases}
\end{align*}
$$

常用对(数几)率函数替代符号函数

$$
\begin{align*}
    \quad y = \sigma(\wv^\top \xv) = \frac{1}{1 + \exp(-\wv^\top \xv)}
\end{align*}
$$

对率函数$\sigma(z)$有很好的数学性质：

$$
\begin{align*}
    \quad & \sigma(z) = \frac{1}{1 + \exp(-z)} = \frac{\exp(z)}{1 + \exp(z)} = 1 - \sigma(- z) \quad \class{blue}{\text{关于}(0,0.5)\text{旋转对称}} \\
    \quad & \nabla_z \sigma(z) = \nabla_z \frac{1}{1 + \exp(-z)} = \frac{1}{1 + \exp(-z)} \frac{\exp(-z)}{1 + \exp(-z)} = \sigma(z) (1 - \sigma(z))
\end{align*}
$$

@import "../python/logistic-func-plot.svg" {.width40 .left55per .top-56per}

<!-- slide vertical=true data-notes="" -->

##### 对数几率回归

---

{==对率函数输出是连续的$[0,1]$==}，可视为{==后验概率==}估计$\Pbb(y = +1 | \xv)$

$$
\begin{align*}
    \quad \Pbb(y = +1 | \xv) &  = \sigma(\wv^\top \xv) = \frac{1}{1 + \exp(-\wv^\top \xv)} \\
    \Pbb(y = -1 | \xv) & = 1 - \sigma(\wv^\top \xv) = \frac{\exp(-\wv^\top \xv)}{1 + \exp(-\wv^\top \xv)}
\end{align*}
$$

<div class="top-4"></div>

两式相除再取对数可得对(数几)率回归 (<u>l</u>ogistic <u>r</u>egression, LR)

$$
\begin{align*}
    \quad \wv^\top \xv = \ln \frac{\Pbb(y = +1 | \xv)}{\Pbb(y = -1 | \xv)} = \ln \frac{\sigma(\wv^\top \xv)}{1 - \sigma(\wv^\top \xv)}
\end{align*}
$$

- 正类概率与负类概率的比值称为{==几率==} (odds)
- 对率回归，顾名思义就是{==用线性函数拟合几率的对数==} (logit)
- 有人将其译为逻辑(斯蒂)回归，但逻辑 (logic) 与 (logistic) 相去甚远
- 虽然名字里有回归，实际是个分类模型

<!-- slide data-notes="" -->

##### 对率回归求解

---

<div class="top2"></div>

$$
\begin{align*}
    \quad \Pbb(y = +1 | \xv) = \sigma(\wv^\top \xv), \quad \Pbb(y = -1 | \xv) = 1 - \sigma(\wv^\top \xv) = \sigma(-\wv^\top \xv)
\end{align*}
$$

<div class="top-2"></div>

不难发现后验概率可紧凑地写为$\Pbb(y | \xv) = \sigma(y \wv^\top \xv)$

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$，则对数似然函数

$$
\begin{align*}
    \quad \ln \Pbb & (D | \wv) = \ln \prod_{i \in [m]} \Pbb( \xv_i, y_i | \wv) = \const + \ln \prod_{i \in [m]} \Pbb(y_i | \xv_i, \wv) \\
    & = \const + \sum_{i \in [m]} \ln \sigma(y_i \wv^\top \xv_i) = \const + \sum_{i \in [m]} \ln \frac{1}{1 + \exp(- y_i \wv^\top \xv_i)}
\end{align*}
$$

根据极大似然法，对率回归形式化的优化问题为

$$
\begin{align*}
    \quad \min_{\wv} ~ \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i)) = - \sum_{i \in [m]} \ln \sigma(y_i \wv^\top \xv_i)
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 对率回归求解

---

对率回归形式化的优化问题为

$$
\begin{align*}
    \quad \min_{\wv} ~ \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i)) = - \sum_{i \in [m]} \ln \sigma(y_i \wv^\top \xv_i)
\end{align*}
$$

<div class="top-2"></div>

目标函数的{==梯度==} (gradient) 和{==海森矩阵==} (Hessian matrix) 分别为

$$
\begin{align*}
    \quad & \gv = - \sum_{i \in [m]} \frac{\sigma(y_i \wv^\top \xv_i) (1 - \sigma(y_i \wv^\top \xv_i))}{\sigma(y_i \wv^\top \xv_i)} y_i \xv_i = \sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i) - 1) y_i \xv_i \\
    & \Hv = \sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i)) (1 - \sigma(y_i \wv^\top \xv_i)) \xv_i \xv_i^\top
\end{align*}
$$

<div class="top-2"></div>

有了梯度，就可以用一阶优化算法如{==梯度下降法==}进行求解

有了海森矩阵，就可以用二阶优化算法如{==牛顿法==}进行求解

<!-- slide vertical=true data-notes="" -->

##### 对率回归求解

---

若标记集合$\Ycal = \{ 0,1 \}$，记

- $\Pbb(y = 1 | \xv) = \sigma(\wv^\top \xv)$
- $\Pbb(y = 0 | \xv) = 1 - \sigma(\wv^\top \xv) = \sigma(-\wv^\top \xv)$

<div class="top2"></div>

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$，则对数似然函数

$$
\begin{align*}
    \quad \ln \Pbb(D | \wv) & = \ln \prod_{i \in [m]} \Pbb(\xv_i, y_i | \wv) = \const + \ln \prod_{i \in [m]} \Pbb(y_i | \xv_i, \wv) \\
    & = \const + \sum_{i \in [m]} \ln \class{blue}{\sigma(\wv^\top \xv_i)^{y_i} \sigma(-\wv^\top \xv_i)^{1 - y_i}} \\
    & = \const + \sum_{i \in [m]} \left( y_i \ln \frac{\sigma(\wv^\top \xv_i)}{1 - \sigma(\wv^\top \xv_i)} + \ln \sigma(-\wv^\top \xv_i) \right) \\
    & = \const + \sum_{i \in [m]} (y_i \wv^\top \xv_i + \ln \sigma(-\wv^\top \xv_i))
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 对率回归求解

---

根据极大似然法，优化问题为

$$
\begin{align*}
    \quad \min_{\wv} \sum_{i \in [m]} (- \ln \sigma(-\wv^\top \xv_i) - y_i \wv^\top \xv_i)
\end{align*}
$$

<div class="top-2"></div>

目标函数的梯度和海森矩阵分别为

$$
\begin{align*}
    \quad \gv & = \sum_{i \in [m]} \left( - \frac{\sigma(-\wv^\top \xv_i) (1 - \sigma(-\wv^\top \xv_i))}{\sigma(-\wv^\top \xv_i)} (-\xv_i) - y_i \xv_i \right) \\
    & = \sum_{i \in [m]} (\sigma(\wv^\top \xv_i) - y_i) \xv_i \\[4pt]
    \Hv & = \sum_{i \in [m]} \sigma(\wv^\top \xv_i) (1 - \sigma(\wv^\top \xv_i)) \xv_i \xv_i^\top
\end{align*}
$$

<!-- slide data-notes="" -->

##### 另一个视角

---

<div class="threelines row1-column2-border1-left-solid row2-column2-border1-left-solid row3-column1-border1-left-solid row4-column2-border1-left-solid row5-column1-border1-left-solid row6-column2-border1-left-solid row7-column2-border1-left-solid row3-column1-border1-right-dashed row5-column1-border1-right-dashed column3-border-left-dashed column1-border1-right-solid-head row1-border-bottom-dashed row3-border-bottom-dashed row5-border-bottom-dashed row6-border-bottom-dashed head-highlight-1 tr-hover fs13">

|    &emsp;    |                                                 >                                                 |                                        对率回归                                         |
| :----------: | :-----------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------: |
|   $\Ycal$    |                                           $\{ \pm 1 \}$                                           |                                       $\{ 0,1 \}$                                       |
| 后验<br>概率 |                      $\Pbb(y=+1 \big\arrowvert \xv) = \sigma(\wv^\top \xv)$                       |                  $\Pbb(y=1 \big\arrowvert \xv) = \sigma(\wv^\top \xv)$                  |
|      ^       |                      $\Pbb(y=-1 \big\arrowvert \xv) = \sigma(-\wv^\top \xv)$                      |                 $\Pbb(y=0 \big\arrowvert \xv) = \sigma(-\wv^\top \xv)$                  |
| 优化<br>目标 |                $\min_{\wv} \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i))$                 |   $\min_{\wv} \sum_{i \in [m]} (\ln (1 + \exp(\wv^\top \xv_i)) - y_i \wv^\top \xv_i)$   |
|      ^       |                 $\min_{\wv} (- \sum_{i \in [m]} \ln \sigma(y_i \wv^\top \xv_i))$                  |   $\min_{\wv} \sum_{i \in [m]} (- \ln \sigma(-\wv^\top \xv_i) - y_i \wv^\top \xv_i)$    |
|    $\gv$     |                   $\sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i) - 1) y_i \xv_i$                   |                 $\sum_{i \in [m]} (\sigma(\wv^\top \xv_i) - y_i) \xv_i$                 |
|    $\Hv$     | $\sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i)) (1 - \sigma(y_i \wv^\top \xv_i)) \xv_i \xv_i^\top$ | $\sum_{i \in [m]} \sigma(\wv^\top \xv_i) (1 - \sigma(\wv^\top \xv_i)) \xv_i \xv_i^\top$ |

</div>

以上结果是通过{==极大似然法==}导出的

用{==交叉熵==} (cross entropy) 可以导出一致的结果

<!-- slide vertical=true data-notes="" -->

##### 交叉熵

---

问题：给定离散概率分布$\qv$，如何度量分布$\pv$与它之间的差异？

定义交叉熵$H_{\qv} (\pv) \triangleq - \sum_i q_i \log p_i = \sum_i q_i \log (1/p_i)$

当$\pv = \qv$时交叉熵最小，此时交叉熵$H_{\qv} (\pv)$即为分布$\qv$的熵$H(\qv)$

<div class="top1"></div>

$$
\begin{align*}
    \quad \min_{\pv} H_{\qv} (\pv) = - \sum_i q_i \log p_i = \sum_i q_i \log (1/p_i), \quad \st ~ \sum_i p_i = 1
\end{align*}
$$

<div class="top-4"></div>

拉格朗日函数为$L = - \sum_i q_i \log p_i + \alpha (\sum_i p_i - 1)$，于是

<div class="top1"></div>

$$
\begin{align*}
    \quad \nabla_{p_i} L = - \frac{q_i}{p_i} + \alpha = 0 \Longrightarrow q_i = \alpha p_i \Longrightarrow \sum_i q_i = \alpha \sum_i p_i \Longrightarrow \alpha = 1
\end{align*}
$$

- 取$\qv$为真实标记在类别上的分布，$\pv$为预测结果在类别上的分布
- 交叉熵可以看成一种{==损失==} (loss)，越小预测结果和真实标记越接近

<!-- slide vertical=true data-notes="" -->

##### 交叉熵损失

---

- 真实标记分布：$\qv_{\{ \pm 1 \}} = [(1+y)/2; (1-y)/2]$，$\qv_{\{ 0,1 \}} = [y; 1-y]$
- 预测结果分布：$\pv = [\sigma(\wv^\top \xv); \sigma(-\wv^\top \xv)]$

<div class="top4"></div>

$$
\begin{align*}
    \quad H_{\qv_{\{ \pm 1 \}}} (\pv) & = - \frac{1+y}{2} \log \sigma(\wv^\top \xv) - \frac{1-y}{2} \log \sigma(-\wv^\top \xv) \\
    & = \begin{cases}
        - \log \sigma(\wv^\top \xv), & y = 1 \\
        - \log \sigma(-\wv^\top \xv), & y = -1
    \end{cases} \\
    & = - \log \sigma(y \wv^\top \xv) = \log (1 + \exp (- y \wv^\top \xv)) \\[10pt]
    H_{\qv_{\{ 0,1 \}}} (\pv) & = - y \log \sigma(\wv^\top \xv) - (1-y) \log \sigma(-\wv^\top \xv) \\
    & = -y \log \frac{\sigma(\wv^\top \xv)}{1 - \sigma(\wv^\top \xv)} - \log \sigma(-\wv^\top \xv) \\
    & = \log (1 + \exp(\wv^\top \xv)) - y \wv^\top \xv
\end{align*}
$$

<div class="top-2"></div>

我的批注 交叉熵里用的是$\log$，和前面的$\ln$只差个常数系数$\log e$

<!-- slide data-notes="" -->

##### 多分类扩展

---

设共有$c$个类，对率回归的预测结果分布为

$$
\begin{align*}
    \Pbb(y=1|\xv) & = \sigma_1 (\xv) = \frac{\exp(\wv_1^\top \xv)}{\sum_{k \in [c]} \exp(\wv_i^\top \xv)} = \frac{\exp((\wv_1 - \wv_c)^\top \xv)}{1 + \sum_{k \in [c-1]} \exp((\wv_i - \wv_c)^\top \xv)} \\
    \Pbb(y=2|\xv) & = \sigma_2 (\xv) = \frac{\exp(\wv_2^\top \xv)}{\sum_{k \in [c]} \exp(\wv_i^\top \xv)} = \frac{\exp((\wv_2 - \wv_c)^\top \xv)}{1 + \sum_{k \in [c-1]} \exp((\wv_i - \wv_c)^\top \xv)} \\
    & \vdots \\
    \Pbb(y=c|\xv) & = \sigma_c (\xv) = \frac{\exp(\wv_c^\top \xv)}{\sum_{k \in [c]} \exp(\wv_i^\top \xv)} = \frac{1}{1 + \sum_{k \in [c-1]} \exp((\wv_i - \wv_c)^\top \xv)}
\end{align*}
$$

- 前一种写法称为 {==softmax==} 变换，二分类 logistic 激活函数的多分类推广
- 后一种写法只需$\uv_1 = \wv_1 - \wv_c, \ldots, \uv_{c-1} = \wv_{c-1} - \wv_c$这$c-1$个向量
- 若$c=2$，则$\uv_1 = \wv_1 - \wv_2$就是前面二分类对率回归中要学的$\wv$

<!-- slide vertical=true data-notes="" -->

##### 多分类对率回归

---

后验概率$\Pbb(y|\xv) = \sigma_1(\xv)^{\Ibb(y=1)} \cdots \sigma_c(\xv)^{\Ibb(y=c)}$，根据极大似然估计

<div class="top1"></div>

$$
\begin{align*}
    \quad \max_{\wv_1, \ldots, \wv_c} \sum_{i \in [m]} \ln \left( \sigma_1(\xv_i)^{\Ibb(y_i=1)} \cdots \sigma_c(\xv_i)^{\Ibb(y_i=c)} \right) = \sum_{i \in [m]} \ln \sigma_{y_i}(\xv_i)
\end{align*}
$$

真实标记分布为$[\Ibb(y = 1);\ldots;\Ibb(y = c)]$，根据最小化交叉熵损失

$$
\begin{align*}
    \quad \min_{\wv_1, \ldots, \wv_c} \sum_{i \in [m]} \sum_{k \in [c]} \Ibb(y_i = k) \log \frac{1}{\sigma_k(\xv_i)} = \sum_{i \in [m]} \log \frac{1}{\sigma_{y_i}(\xv_i)}
\end{align*}
$$

我的批注 {==极大似然估计==}和{==最小化交叉熵损失==}再次取得了一致

<!-- slide data-notes="" -->

##### 多分类对率回归求解

---

注意

$$
\begin{align*}
    ~ \sigma_{y_i} (\xv_i) = \frac{\exp(\wv_{y_i}^\top \xv_i)}{\sum_{k \in [c]} \exp(\wv_k^\top \xv_i)}
\end{align*}
$$

<div class="top-4"></div>

于是

$$
\begin{align*}
    ~ \nabla_{\wv_{y_i}} \sigma_{y_i} (\xv_i) & = \frac{\exp(\wv_{y_i}^\top \xv_i) \xv_i \sum_{k \in [c]} \exp(\wv_k^\top \xv_i) - \exp(\wv_{y_i}^\top \xv_i) \exp(\wv_{y_i}^\top \xv_i) \xv_i}{(\sum_{k \in [c]} \exp(\wv_k^\top \xv_i))^2} \\[4pt]
    & = \sigma_{y_i} (\xv_i) \xv_i - \sigma_{y_i}^2 (\xv_i) \xv_i = \sigma_{y_i} (\xv_i) (1 - \sigma_{y_i} (\xv_i)) \xv_i \\[10pt]
    \nabla_{\wv_l} \sigma_{y_i} (\xv_i) & = \frac{- \exp(\wv_{y_i}^\top \xv_i) \exp(\wv_l^\top \xv_i) \xv_i}{(\sum_{k \in [c]} \exp(\wv_k^\top \xv_i))^2} = - \sigma_{y_i} (\xv_i) \sigma_l (\xv_i) \xv_i, ~ l \ne y_i
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 多分类对率回归求解

---

<div class="top2"></div>

$$
\begin{align*}
    \qquad & \nabla_{\wv_{y_i}} \sigma_{y_i} (\xv_i) = \class{blue}{\sigma_{y_i} (\xv_i) (1 - \sigma_{y_i} (\xv_i)) \xv_i} \\
    & \nabla_{\wv_l} \sigma_{y_i} (\xv_i) = \class{yellow}{- \sigma_{y_i} (\xv_i) \sigma_l (\xv_i) \xv_i}, \quad l \ne y_i \\[16pt]
    \quad & \nabla_{\wv_l} \left( \sum_{i \in [m]} \ln \sigma_{y_i}(\xv_i) \right) = \sum_{i \in [m]} \frac{\nabla_{\wv_l} \sigma_{y_i} (\xv_i)}{\sigma_{y_i}(\xv_i)} \\
    = & \sum_{i: y_i = l} \frac{\nabla_{\wv_l} \sigma_{y_i} (\xv_i)}{\sigma_{y_i}(\xv_i)} + \sum_{i: y_i \neq l} \frac{\nabla_{\wv_l} \sigma_{y_i} (\xv_i)}{\sigma_{y_i}(\xv_i)} \\
    = & \sum_{i: y_i = l} \frac{\class{blue}{\sigma_l (\xv_i) (1 - \sigma_l (\xv_i))\xv_i}}{\sigma_l (\xv_i)} + \sum_{i: y_i \neq l} \frac{\class{yellow}{-\sigma_{y_i} (\xv_i) \sigma_l (\xv_i) \xv_i}}{\sigma_{y_i} (\xv_i)} \\
    = & \sum_{i: y_i = l} (1 - \sigma_l (\xv_i)) \xv_i - \sum_{i: y_i \neq l} \sigma_l (\xv_i) \xv_i = \sum_{i: y_i = l} \xv_i - \sum_{i \in [m]} \sigma_l (\xv_i) \xv_i \\
    = & \sum_{i \in [m]} (\Ibb(y_i=l) - \sigma_l (\xv_i)) \xv_i
\end{align*}
$$

<!-- slide data-notes="" -->

##### <span style="font-weight:900">sklearn</span> 中的对率回归

---

```python {.line-numbers .top-1 .left4 highlight=[2,18,21-27]}
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelBinarizer, OneHotEncoder

X = np.array([[1, '周六', '吃饭', '晴天', '轻松', '清零', '精彩'], ..., [17, '周六', '吃饭', '阴天', '适中', '平缓', '精彩'],])
y = np.array(['是', '是', '是', '是', '是', '是', '是', '是', '否', '否', '否', '否', '否', '否', '否', '否', '否'])
X = OneHotEncoder().fit_transform(X[:, 1:7])
y = LabelBinarizer().fit_transform(y).squeeze()

train_index = [0, 1, 2, 5, 6, 9, 13, 14, 15, 16]
test_index = [3, 4, 7, 8, 10, 11, 12]
X_train, X_test = X[train_index, :], X[test_index, :]
y_train, y_test = y[train_index], y[test_index]

clf = LogisticRegression(penalty='none', verbose=True)  # 无正则项 输出日志
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
0.7142857142857143

np.hstack((clf.predict_proba(X_test), OneHotEncoder().fit_transform(y_test[:, np.newaxis]).toarray()))
[[3.19744231e-14, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00],
 [3.45002693e-10, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00],
 [9.99999999e-01, 8.41179268e-10, 0.00000000e+00, 1.00000000e+00],
 [1.00000000e+00, 4.23642522e-19, 1.00000000e+00, 0.00000000e+00],
 [1.00000000e+00, 2.94646658e-47, 1.00000000e+00, 0.00000000e+00],
 [1.00000000e+00, 5.95930974e-17, 1.00000000e+00, 0.00000000e+00],
 [0.00000000e+00, 1.00000000e+00, 1.00000000e+00, 0.00000000e+00]]
```

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">sklearn</span> 中的对率回归

---

```python {.line-numbers .top-1 .left4 highlight=[3,14,17-19,22-24]}
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = LogisticRegression(penalty='none', verbose=True)  # 无正则项 输出日志
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
1.0

clf.predict_proba(X_test)
[[1.88995213e-15, 9.99887032e-01, 1.12968186e-04],
 ...
 [9.99999994e-01, 6.37873593e-09, 6.77211243e-31]]

OneHotEncoder().fit_transform(y_test[:, np.newaxis]).toarray()
[[0., 1., 0.],
 ...
 [1., 0., 0.]]
```

<!-- slide data-notes="" -->

##### 核对率回归

---

以二分类为例，引入特征映射$\phi(\cdot)$可得原始形式的核对率回归：

<div class="top1"></div>

$$
\begin{align*}
    \quad \min_{\wv} \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i)) ~ \longrightarrow ~ \min_{\wv} \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \phi(\xv_i)))
\end{align*}
$$

问题：如何让模型中只出现内积$\phi(\xv_i)^\top \phi(\xv_j)$的形式？

正交分解$\wv = \sum_{i \in [m]} \alpha_i \phi(\xv_i) + \vv$，其中对$\forall i \in [m]$有$\vv \perp \phi(\xv_i)$

注意$\kappa(\xv_j, \xv_i) = \phi(\xv_j)^\top \phi(\xv_i)$，核对率回归的对偶形式为

$$
\begin{align*}
    \quad \min_{\wv} \sum_{i \in [m]} \ln \left( 1 + \exp \left( -y_i \sum_{j \in [m]} \alpha_j \kappa(\xv_j, \xv_i) \right) \right)
\end{align*}
$$

<!-- slide data-notes="" -->

##### 优化

---

<div class="threelines head-highlight-1 tr-hover column3-border1-right-solid-head row1-border-bottom-dashed row3-border-bottom-dashed column2-border-left-solid column3-border-left-solid row1-column4-border1-left-solid row2-column4-border1-left-solid row3-column1-border1-left-solid row4-column4-border1-left-solid row5-column1-border1-left-solid fs13">

|   模型   |    $\Ycal$    |                                      优化目标                                      |                             $\gv$                             |
| :------: | :-----------: | :--------------------------------------------------------------------------------: | :-----------------------------------------------------------: |
| 线性回归 |    $\Rbb$     |               $\min_{\wv} \sum_{i \in [m]}(\wv^\top \xv_i - y_i)^2$                |       $2 \sum_{i \in [m]} (\wv^\top \xv_i - y_i) \xv_i$       |
|  感知机  | $\{ \pm 1 \}$ |          $\min_{\wv} \sum_{i \in [m]}\max \{ 0, - y_i \wv^\top \xv_i \}$           | $-\sum_{i \in [m]} \Ibb(y_i \wv^\top \xv_i \le 0) y_i \xv_i$  |
|    ^     |   $\{1,0\}$   |      $\min_{\wv} \sum_{i \in [m]}(\sgn(\wv^\top \xv_i) - y_i) \wv^\top \xv_i$      |     $\sum_{i \in [m]} (\sgn(\wv^\top \xv_i) - y_i) \xv_i$     |
| 对率回归 | $\{ \pm 1 \}$ |         $\min_{\wv} \sum_{i \in [m]}\ln (1 + \exp(- y_i \wv^\top \xv_i))$          | $\sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i) - 1) y_i \xv_i$ |
|    ^     |  $\{ 1,0 \}$  | $\min_{\wv} \sum_{i \in [m]}(\ln (1 + \exp(\wv^\top \xv_i)) - y_i \wv^\top \xv_i)$ |    $\sum_{i \in [m]} (\sigma(\wv^\top \xv_i) - y_i) \xv_i$    |

</div>

求极值通常来说只需令梯度$\gv = \zerov$即可，但$\wv$可能会很难求

- 线性回归对应的$\gv = \zerov$有解析解$\wv^\star = (\sum_{i \in [m]} \xv_i \xv_i^\top)^{-1} (\sum_{i \in [m]} y_i \xv_i)$
- 感知机和对率回归对应的$\gv = \zerov$是非线性方程，更难求解

<!-- slide vertical=true data-notes="" -->

##### 梯度下降

---

设优化目标函数为$F(\wv)$，用迭代法构造单调递减序列：

$$
\begin{align*}
    \quad F(\wv_0) \ge F(\wv_1) \ge F(\wv_2) \ge F(\wv_3) \ge \cdots
\end{align*}
$$

<div class="top-3"></div>

我的启示 {==单调有界序列必收敛==}，若$F$有下界，序列就会收敛到$F(\wv^\star)$

问题：如何实现单调递减？根据泰勒展开式及柯西不等式知

$$
\begin{align*}
    -\| \nabla F(\wv)\| ~ \|\uv\| \le \lim_{\eta \rightarrow 0} \frac{F(\wv + \eta \uv) - F(\wv)}{\eta} = \nabla F(\wv)^\top \uv \le \| \nabla F(\wv)\| ~ \|\uv\|
\end{align*}
$$

<div class="top-1"></div>

- 只要$\uv$与$\nabla F(\wv)$夹角为钝角，就可以使得目标函数值下降
- 左边取等号条件是$\uv = -k \cdot \nabla F(\wv)$，{==负梯度是瞬时下降最快的方向==}

<div class="top2"></div>

梯度下降 (<u>g</u>radient <u>d</u>escent, GD)：$\wv_{t+1} \leftarrow \wv_t - \eta_t \nabla F(\wv_t)$

<!-- slide vertical=true data-notes="" -->

##### 梯度下降解线性回归

---

@import "../python/gd.svg" {.center .top6 .width90 title="用梯度下降求解 R 上的最小二乘"}

<!-- slide data-notes="" -->

##### 优化 损失函数的和

---

<div class="threelines head-highlight-1 tr-hover column3-border1-right-solid-head row1-border-bottom-dashed row3-border-bottom-dashed column2-border-left-solid column3-border-left-solid row1-column4-border1-left-solid row2-column4-border1-left-solid row3-column1-border1-left-solid row4-column4-border1-left-solid row5-column1-border1-left-solid bottom-2 fs13">

|   模型   |    $\Ycal$    |                                      优化目标                                      |                             $\gv$                             |
| :------: | :-----------: | :--------------------------------------------------------------------------------: | :-----------------------------------------------------------: |
| 线性回归 |    $\Rbb$     |               $\min_{\wv} \sum_{i \in [m]}(\wv^\top \xv_i - y_i)^2$                |       $2 \sum_{i \in [m]} (\wv^\top \xv_i - y_i) \xv_i$       |
|  感知机  | $\{ \pm 1 \}$ |          $\min_{\wv} \sum_{i \in [m]}\max \{ 0, - y_i \wv^\top \xv_i \}$           | $-\sum_{i \in [m]} \Ibb(y_i \wv^\top \xv_i \le 0) y_i \xv_i$  |
|    ^     |   $\{1,0\}$   |      $\min_{\wv} \sum_{i \in [m]}(\sgn(\wv^\top \xv_i) - y_i) \wv^\top \xv_i$      |     $\sum_{i \in [m]} (\sgn(\wv^\top \xv_i) - y_i) \xv_i$     |
| 对率回归 | $\{ \pm 1 \}$ |         $\min_{\wv} \sum_{i \in [m]}\ln (1 + \exp(- y_i \wv^\top \xv_i))$          | $\sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i) - 1) y_i \xv_i$ |
|    ^     |  $\{ 1,0 \}$  | $\min_{\wv} \sum_{i \in [m]}(\ln (1 + \exp(\wv^\top \xv_i)) - y_i \wv^\top \xv_i)$ |    $\sum_{i \in [m]} (\sigma(\wv^\top \xv_i) - y_i) \xv_i$    |

</div>

许多机器学习模型的优化目标为{==最小化每个样本的损失的和==}

$$
\begin{align*}
    \quad \min_{\wv} F(\wv) = \frac{1}{m} \sum_{i \in [m]} \ell(y_i, f(\xv_i;\wv))
\end{align*}
$$

<div class="top-3"></div>

- 线性回归，平方损失$\ell = (y_i - \wv^\top \xv_i)^2$
- 感知机，$\ell = \max \{ 0, - y_i \wv^\top \xv_i \}$
- 对率回归，对率损失$\ell = \ln (1 + \exp(- y_i \wv^\top \xv_i))$，更一般的交叉熵损失

<!-- slide vertical=true data-notes="" -->

##### 随机梯度下降

---

<div class="top2"></div>

- 计算梯度$\nabla F(\wv) = \sum_{i \in [m]} \nabla \ell(y_i, f(\xv_i;\wv)) / m$需遍历所有样本
- 当样本数$m$很大时，计算开销很大

<div class="top2"></div>

批量梯度下降随机采样一个下标子集$\Bcal_t \subseteq [m]$

$$
\begin{align*}
    \quad \wv_{t+1} \leftarrow \wv_t - \eta_t \frac{1}{|\Bcal_t|} \sum_{i \in \Bcal_t} \nabla \ell(y_i, f(\xv_i;\wv))
\end{align*}
$$

<div class="top-5"></div>

若$|\Bcal_t| = 1$，则为标准的{==随机梯度下降==} (<u>s</u>tochastic <u>GD</u>, SGD)

以感知机为例：

- 优化目标为$\min_{\wv} \sum_{i \in [m]} \max \{ 0, - y_i \wv^\top \xv_i \}$
- 梯度为$\gv = - \sum_{i \in [m]} \Ibb(y_i \wv^\top \xv_i \le 0) y_i \xv_i$
- 随机梯度下降$\wv_{t+1} \leftarrow \wv_t + \eta_t \Ibb(y_i \wv^\top \xv_i \le 0) y_i \xv_i$

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">GD</span> _vs._ <span style="font-weight:900">SGD</span>

---

更新公式：

$$
\begin{align*}
    \quad & \wv_{t+1} \leftarrow \wv_t - \eta_t \frac{1}{m} \sum_{i \in [m]} \nabla \ell(y_i, f(\xv_i;\wv)) \\
    & \wv_{t+1} \leftarrow \wv_t - \eta_t \frac{1}{|\Bcal_t|} \sum_{i \in \Bcal_t} \nabla \ell(y_i, f(\xv_i;\wv))
\end{align*}
$$

<div class="top-2"></div>

- 当数据集中有冗余样本时，SGD 可以减少重复计算
- 迭代前期，SGD 更新频率快，较 GD 优势明显
- 迭代后期，GD 会停止于最优解处，SGD 则只能在最优解附近震荡
- 越靠近最优解，梯度越接近零，因此 GD 可以用恒定步长
- 最优解处随机梯度不一定为零，故 SGD 必须用衰减步长
- SGD 因随机采样带来的噪声若能随着迭代而受到抑制，则可进一步加速，机器学习顶会有大量相关工作，包括 SAG，SAGA，SVRG 等及其衍生变种

<!-- slide data-notes="" -->

##### 加速梯度下降

---

当目标函数的变量尺度不同时，梯度下降效率很低

@import "../python/momentum.svg" {.center .width90 .top0 .bottom0}

动量法 (momentum)：$\wv_{t+1} = \wv_t - \eta_t \nabla F(\wv_t) + \class{blue}{\gamma (\wv_t - \wv_{t-1})}$

- 相对于梯度下降，多了第三项，上一轮的更新方向
- 参数$\gamma < 1$，通常取$0.9$

<!-- slide vertical=true data-notes="" -->

##### 动量法

---

$$
\begin{align*}
    \qquad \wv_{t+1} - \wv_t & = - \eta_t \nabla F(\wv_t) + \gamma (\wv_t - \wv_{t-1}) \\
    \gamma (\wv_t - \wv_{t-1}) & = - \eta_{t-1} \gamma \nabla F(\wv_{t-1}) + \gamma^2 (\wv_{t-1} - \wv_{t-2}) \\
    & \vdots \\
    \gamma^{t-1} (\wv_2 - \wv_1) & = - \eta_1 \gamma^{t-1} \nabla F(\wv_1) + \underbrace{\gamma^t (\wv_1 - \wv_0)}_{\wv_1 ~ = ~ \wv_0} \\
    \Longrightarrow ~ \wv_{t+1} - \wv_t & = - \sum_{i \in [t]} \eta_i \gamma^{t-i} \nabla F(\wv_i)
\end{align*}
$$

<div class="top-4"></div>

动量法每步更新是历史梯度的加权平均

- 若近期梯度方向不一致，则真实的更新幅度变小，减速，增加稳定性
- 若近期梯度方向较为一致，则真实的更新幅度变大，加速，加快收敛

Nesterov 加速梯度 (NAG)：改进动量法的第二步

$$
\begin{align*}
    \begin{cases} \widetilde{\wv} = \wv_t + \gamma (\wv_t - \wv_{t-1}) \\ \wv_{t+1} = \widetilde{\wv} - \eta_t \class{yellow}{\nabla F (\wv_t)} \end{cases} ~ \longrightarrow ~ \begin{cases} \widetilde{\wv} = \wv_t + \gamma (\wv_t - \wv_{t-1}) \\ \wv_{t+1} = \widetilde{\wv} - \eta_t \class{yellow}{\nabla F (\widetilde{\wv})} \end{cases}
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 动量法 <span style="font-weight:900">_vs._ NAG</span>

---

第$t$轮迭代示意图：

@import "../tikz/mvsnag.svg" {.center .width90 .top6 .bottom2}

<!-- slide data-notes="" -->

##### 本章小结

---

对率回归采用{==对率函数==}作为激活函数，输出具有{==概率==}意义

对率回归虽名为回归，但实际是个{==线性分类==}模型

对率回归最终形式化可由{==极大似然法==}或{==最小化交叉熵==}损失得到

对率回归可以很自然地推广到多分类问题上

对率回归的求解

- 一阶优化方法如{==梯度下降法==}及其变种
- 二阶优化方法如牛顿法及其变种
