---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"
@import "https://cdn.bootcdn.net/ajax/libs/jquery/3.5.0/jquery.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 支持向量机

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 大纲

---

@import "../vega/outline.json" {as="vega" .top-2}

<!-- slide vertical=true data-notes="" -->

##### 发明人

---

Vladimir Vapnik：苏联统计学家

<div class="top-3"></div>

Corinna Cortes：纽约 Google Research 的负责人

<div>
    <img src="../img/svm/Vladimir Vapnik.jpg" title="Vladimir Vapnik" width=380px>
    <img src="../img/svm/Corinna Cortes.jpg" title="Corinna Cortes" width=380px class="left4">
</div>

<!-- slide data-notes="" -->

##### 楚河汉界 间隔

---

<div id="board2" class="center" style="width:420px"></div>

<div class="top-33per left-70per bottom-10">
<button id="startBtn" class="top-40per">开始</button>
<button id="clearBtn">清空</button>
</div>

@import "../js/xiangqiboardjs-0.3.3/css/xiangqiboard-0.3.3.css"
@import "../js/xiangqiboardjs-0.3.3/js/xiangqiboard-0.3.3.js"
@import "../js/xiangqiboardjs-0.3.3/js/svm-chess.js"

<!-- slide vertical=true data-notes="" -->

##### 最大间隔准则

---

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Rbb^d \times \{ \pm 1 \})^m$，最大化间隔：

<div class="top2"></div>

$$
\begin{align*}
    \quad & \max_{\wv,b} ~ \gamma, \quad \st ~ \frac{y_i (\wv^\top \xv_i + b)}{\|\wv\|} \ge \gamma, ~ \forall i \in [m] \\[2pt]
    & \qquad \qquad \qquad \Updownarrow \\[2pt]
    & \max_{\wv,b} ~ \frac{\hat{\gamma}}{\|\wv\|}, \quad \st ~ y_i (\wv^\top \xv_i + b) \ge \hat{\gamma}, ~ \forall i \in [m] \\[2pt]
    & \qquad \qquad \qquad \Updownarrow \\[2pt]
    & \max_{\wv,b} ~ \frac{1}{\|\wv\|}, \quad \st ~ y_i (\wv^\top \xv_i + b) \ge 1, ~ \forall i \in [m] \\[2pt]
    & \qquad \qquad \qquad \Updownarrow \\[2pt]
    & \min_{\wv,b} ~ \frac{1}{2} \|\wv\|_2^2, \quad \st ~ y_i (\wv^\top \xv_i + b) \ge 1, ~ \forall i \in [m]
\end{align*}
$$

<!-- slide data-notes="" -->

##### 支持向量机

---

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Rbb^d \times \{ \pm 1 \})^m$

根据最大间隔准则导出支持向量机：

$$
\begin{align*}
    \quad \min_{\wv,b} ~ \frac{1}{2} \|\wv\|_2^2, \quad \st ~ y_i (\wv^\top \xv_i + b) \ge 1, ~ \forall i \in [m]
\end{align*}
$$

<div class="top-2"></div>

分解$\wv = \sum_{i \in [m]} \alpha_i \xv_i + \vv = \Xv^\top \alphav + \vv$，$\vv \perp \span \{ \xv_1, \ldots, \xv_m \}$

- $\wv^\top \xv_i = \alpha^\top \Xv \xv_i$，即约束与$\vv$无关
- $\|\wv\|_2^2 = \alphav^\top \Xv \Xv^\top \alphav + \vv^\top \vv$，故必有$\vv = \zerov$
- {==最优的$\wv$只由训练样本线性表出==}，$\wv = \sum_{i \in [m]} \alpha_i \xv_i = \Xv^\top \alphav$
- 非零线性表出系数$\alpha_i$对应的样本称为{==支持向量==} (<u>s</u>upport <u>v</u>ector, SV)
- $\wv$只与部分支持向量有关，故名{==支持向量机==} (<u>SV</u> <u>m</u>achine, SVM)

<!-- slide vertical=true data-notes="" -->

##### 核支持向量机

---

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Rbb^d \times \{ \pm 1 \})^m$

根据最大间隔准则导出支持向量机：

$$
\begin{align*}
    \quad \min_{\wv,b} ~ \frac{1}{2} \|\wv\|_2^2, \quad \st ~ y_i (\wv^\top \xv_i + b) \ge 1, ~ \forall i \in [m]
\end{align*}
$$

若数据非线性可分怎么办？核支持向量机

$$
\begin{align*}
    \quad \min_{\wv,b} ~ \frac{1}{2} \|\wv\|_2^2, \quad \st ~ y_i (\wv^\top \phi(\xv_i) + b) \ge 1, ~ \forall i \in [m]
\end{align*}
$$

<div class="top-2"></div>

{==最优的$\wv$依然只由训练样本线性表出==}：$\wv = \sum_{i \in [m]} \alpha_i \phi(\xv_i)$

<div class="top-2"></div>

回代可以发现$\phi(\xv_i)$总是成对以内积的形式出现，故可用核函数

<!-- slide vertical=true data-notes="" -->

##### 核支持向量机

---

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Rbb^d \times \{ \pm 1 \})^m$

根据最大间隔准则导出支持向量机：

$$
\begin{align*}
    \quad \min_{\wv,b} ~ \frac{1}{2} \|\wv\|_2^2, \quad \st ~ y_i (\wv^\top \phi(\xv_i) + b) \ge 1, ~ \forall i \in [m]
\end{align*}
$$

问题：

- 很难确定什么样的核映射能保证样本在新的特征空间线性可分
- 即便恰好找到了这样的核映射，如何确定其没有引起过拟合？

<div class="top2"></div>

方案：允许约束$y_i (\wv^\top \phi(\xv_i) + b) \ge 1$对少数样本不成立

<!-- slide data-notes="" -->

##### 软间隔支持向量机

---

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Rbb^d \times \{ \pm 1 \})^m$

根据最大间隔准则导出支持向量机：

$$
\begin{align*}
    \quad \min_{\wv,b} ~ \frac{1}{2} \|\wv\|_2^2, \quad \st ~ y_i (\wv^\top \phi(\xv_i) + b) \ge 1, ~ \forall i \in [m]
\end{align*}
$$

现允许约束$y_i (\wv^\top \phi(\xv_i) + b) \ge 1$对少数样本不成立

$$
\begin{align*}
    \quad \min_{\wv,b} ~ \frac{\lambda}{2} \|\wv\|_2^2 + \frac{1}{m} \underbrace{\sum_{i \in [m]} \Ibb(y_i (\wv^\top \phi(\xv_i) + b) < 1) }_{\text{破坏约束的样本个数}}
\end{align*}
$$

<div class="top-2"></div>

难点：指示函数$\Ibb(\cdot)$<span class="blue">非凸非连续</span>，导致问题很难求解

<!-- slide vertical=true data-notes="" -->

##### 软间隔支持向量机

---

用 hinge 损失代替指示函数可得{==软间隔支持向量机==}：

<div class="top1"></div>

$$
\begin{align*}
    \quad \min_{\wv,b} ~ \frac{\lambda}{2} \|\wv\|_2^2 + \frac{1}{m} \sum_{i \in [m]} \max \{ 0, 1- y_i (\wv^\top \phi(\xv_i) + b) \}
\end{align*}
$$

<div class="top-2"></div>

令$\epsilon_i = \max \{ 0, 1- y_i (\wv^\top \phi(\xv_i) + b) \}$可得其等价约束形式：

<div class="top1"></div>

$$
\begin{align*}
    \quad \min_{\wv,b} & ~ \frac{\lambda}{2} \|\wv\|_2^2 + \frac{1}{m} \sum_{i \in [m]} \epsilon_i \\[2pt]
    \st & ~ y_i (\wv^\top \phi(\xv_i) + b) \ge 1 - \epsilon_i, ~ \epsilon_i \ge 0, ~ \forall i \in [m]
\end{align*}
$$

对比前面的{==硬间隔支持向量机==}可以发现就多了$\epsilon_i$

$\epsilon_i$也叫{==松弛变量==} (slack variable)，刻画了约束被破坏的程度

<!-- slide data-notes="" -->

##### 正则化项 + 损失函数

---

<div class="top2"></div>

$$
\begin{align*}
    \quad \min_{\wv,b} ~ \frac{\lambda}{2} \underbrace{\|\wv\|_2^2}_{\text{正则化项}} + \frac{1}{m} \sum_{i \in [m]} \underbrace{\max \{ 0, 1- y_i (\wv^\top \phi(\xv_i) + b) \}}_{\text{损失函数}}
\end{align*}
$$

- $\ell_2$正则$\| \wv \|_2^2$，得到稠密的$\wv$
- $\ell_1$正则$\| \wv \|_1$，得到稀疏的$\wv$，附带特征选择的作用
- $\ell_\infty$正则$\| \wv \|_\infty$，得到所有分量值相同的$\wv$
- $\ell_{2,1}$正则$\sum_j \| \wv_j \|_2$，特征分组，组内稠密，组间稀疏
- $\ell_{1,2}$正则$(\sum_j \| \wv_j \|_1)^2$，特征分组，组内稀疏，组间稠密
- 弹性网：$\ell_1$正则和$\ell_2$正则的线性组合
- OSCAR：$\ell_1$正则和成对$\ell_\infty$正则的线性组合

<!-- slide vertical=true data-notes="" -->

##### 正则化项 + 损失函数

---

<div class="top2"></div>

$$
\begin{align*}
    \quad \min_{\wv,b} ~ \frac{\lambda}{2} \underbrace{\|\wv\|_2^2}_{\text{正则化项}} + \frac{1}{m} \sum_{i \in [m]} \underbrace{\max \{ 0, 1- y_i (\wv^\top \phi(\xv_i) + b) \}}_{\text{损失函数}}
\end{align*}
$$

- hinge 损失：$l(y, f(\xv)) = \max \{ 0, 1 - y f(\xv) \}$，软间隔支持向量机
- 平方 hinge 损失：$l(y, f(\xv)) = [\max \{ 0, 1 - y f(\xv) \}]^2$
- 平方损失：$l(y, f(\xv)) = (y - f(\xv))^2$，岭回归
- $\epsilon$-不敏感损失：$l(y, f(\xv)) = \max \{ 0, |y - f(\xv)| - \epsilon \}$，支持向量回归
- 指数损失：$l(y, f(\xv)) = \exp (- y f(\xv))$
- 对率损失：$l(y, f(\xv)) = \log (1 + \exp (- y f(\xv)))$，对率回归

<!-- slide vertical=true data-notes="" -->

##### 损失函数

---

@import "../python/surrogate-loss.svg" {title="各种替代损失函数" .center .top4 .width70}
